# -*- coding: utf-8 -*-
"""æŸ¥è¯¢ç”Ÿæˆå·¥å…·"""

from typing import Dict, List, Any, Optional, Union
import re
import json
from datetime import datetime, timedelta
import structlog
from mcp.types import Tool, TextContent

from database.connection_manager import ConnectionManager
from database.metadata_manager import MetadataManager
from scanner.semantic_analyzer import SemanticAnalyzer
from mcp_tools.unified_semantic_tool import UnifiedSemanticTool
from utils.parameter_validator import (
    ParameterValidator, MCPParameterHelper, ValidationResult,
    is_non_empty_string, is_positive_integer, is_boolean, 
    is_valid_instance_id, is_valid_database_name, is_valid_collection_name,
    validate_instance_exists, validate_database_exists, validate_collection_exists
)
from utils.tool_context import get_context_manager, ToolExecutionContext
from utils.error_handler import with_error_handling, with_retry, RetryConfig


logger = structlog.get_logger(__name__)


class QueryGenerationTool:
    """æŸ¥è¯¢ç”Ÿæˆå·¥å…·"""
    
    def __init__(self, connection_manager: ConnectionManager, 
                 metadata_manager: MetadataManager,
                 semantic_analyzer: SemanticAnalyzer):
        self.connection_manager = connection_manager
        self.metadata_manager = metadata_manager
        self.semantic_analyzer = semantic_analyzer
        self.unified_semantic = UnifiedSemanticTool(
            connection_manager, metadata_manager, semantic_analyzer
        )
        self.context_manager = get_context_manager()
        self.validator = self._setup_validator()
        
        # æŸ¥è¯¢æ¨¡å¼æ˜ å°„
        self.query_patterns = {
            # æŸ¥æ‰¾æ¨¡å¼
            r'æŸ¥æ‰¾|æ‰¾åˆ°|è·å–|æœç´¢|æŸ¥è¯¢': 'find',
            r'ç»Ÿè®¡|è®¡æ•°|æ•°é‡|å¤šå°‘': 'count',
            r'èšåˆ|åˆ†ç»„|æ±‡æ€»|ç»Ÿè®¡åˆ†æ': 'aggregate',
            r'å»é‡|å”¯ä¸€|ä¸é‡å¤': 'distinct',
            
            # æ¡ä»¶æ¨¡å¼
            r'ç­‰äº|æ˜¯|ä¸º': '$eq',
            r'ä¸ç­‰äº|ä¸æ˜¯|ä¸ä¸º': '$ne',
            r'å¤§äº': '$gt',
            r'å¤§äºç­‰äº|ä¸å°äº': '$gte',
            r'å°äº': '$lt',
            r'å°äºç­‰äº|ä¸å¤§äº': '$lte',
            r'åŒ…å«|å«æœ‰': '$regex',
            r'åœ¨.*ä¹‹é—´|èŒƒå›´': '$range',
            r'å­˜åœ¨|æœ‰': '$exists',
            r'ä¸å­˜åœ¨|æ²¡æœ‰': '$not_exists',
            r'ä¸ºç©º|ç©ºå€¼': '$null',
            r'ä¸ä¸ºç©º|éç©º': '$not_null',
        }
        
        # æ—¶é—´å…³é”®è¯
        self.time_keywords = {
            r'ä»Šå¤©|å½“å¤©': 0,
            r'æ˜¨å¤©': -1,
            r'å‰å¤©': -2,
            r'æ˜å¤©': 1,
            r'åå¤©': 2,
            r'æœ¬å‘¨|è¿™å‘¨': 'this_week',
            r'ä¸Šå‘¨|ä¸Šä¸€å‘¨': 'last_week',
            r'æœ¬æœˆ|è¿™ä¸ªæœˆ': 'this_month',
            r'ä¸Šæœˆ|ä¸Šä¸ªæœˆ': 'last_month',
            r'ä»Šå¹´|æœ¬å¹´': 'this_year',
            r'å»å¹´|ä¸Šå¹´': 'last_year',
        }
    
    def get_tool_definition(self) -> Tool:
        """è·å–å·¥å…·å®šä¹‰"""
        return Tool(
            name="generate_query",
            description="æ ¹æ®è‡ªç„¶è¯­è¨€æè¿°ç”ŸæˆMongoDBæŸ¥è¯¢è¯­å¥",
            inputSchema={
                "type": "object",
                "properties": {
                    "instance_id": {
                        "type": "string",
                        "description": "MongoDBå®ä¾‹ID"
                    },
                    "database_name": {
                        "type": "string",
                        "description": "æ•°æ®åº“åç§°"
                    },
                    "collection_name": {
                        "type": "string",
                        "description": "é›†åˆåç§°"
                    },
                    "query_description": {
                        "type": "string",
                        "description": "æŸ¥è¯¢çš„è‡ªç„¶è¯­è¨€æè¿°"
                    },
                    "query_type": {
                        "type": "string",
                        "enum": ["auto", "find", "count", "aggregate", "distinct"],
                        "description": "æŸ¥è¯¢ç±»å‹ï¼Œautoè¡¨ç¤ºè‡ªåŠ¨è¯†åˆ«",
                        "default": "auto"
                    },
                    "limit": {
                        "type": "integer",
                        "description": "ç»“æœé™åˆ¶æ•°é‡",
                        "default": 100,
                        "minimum": 1,
                        "maximum": 1000
                    },
                    "include_explanation": {
                        "type": "boolean",
                        "description": "æ˜¯å¦åŒ…å«æŸ¥è¯¢è§£é‡Š",
                        "default": True
                    },
                    "output_format": {
                        "type": "string",
                        "enum": ["full", "query_only", "executable"],
                        "description": "è¾“å‡ºæ ¼å¼ï¼šfull=å®Œæ•´è§£é‡Šï¼Œquery_only=ä»…æŸ¥è¯¢è¯­å¥ï¼Œexecutable=å¯ç›´æ¥æ‰§è¡Œçš„è¯­å¥",
                        "default": "full"
                    }
                },
                "required": ["instance_id", "database_name", "collection_name", "query_description"]
            }
        )
    
    def _setup_validator(self) -> ParameterValidator:
        """è®¾ç½®å‚æ•°éªŒè¯å™¨"""
        validator = ParameterValidator()
        
        async def get_instance_options():
            """è·å–å¯ç”¨å®ä¾‹é€‰é¡¹"""
            try:
                instances = await self.connection_manager.get_all_instances()
                return [{'value': instance_id, 'display_name': instance_id, 
                        'description': instance_config.description or 'æ— æè¿°'} 
                       for instance_id, instance_config in instances.items()]
            except Exception:
                return []
        
        async def get_database_options(context):
            """è·å–å¯ç”¨æ•°æ®åº“é€‰é¡¹"""
            try:
                if not context or not context.instance_id:
                    return []
                
                connection = self.connection_manager.get_instance_connection(context.instance_id)
                if not connection:
                    return []
                
                db_names = await connection.client.list_database_names()
                # è¿‡æ»¤ç³»ç»Ÿæ•°æ®åº“
                system_dbs = {'admin', 'local', 'config'}
                user_dbs = [name for name in db_names if name not in system_dbs]
                
                return [{'value': db_name, 'display_name': db_name,
                        'description': f'æ•°æ®åº“: {db_name}'} for db_name in user_dbs]
            except Exception:
                return []
        
        async def get_collection_options(context):
            """è·å–å¯ç”¨é›†åˆé€‰é¡¹"""
            try:
                if not context or not context.instance_id or not context.database_name:
                    return []
                
                connection = self.connection_manager.get_instance_connection(context.instance_id)
                if not connection:
                    return []
                
                db = connection.client[context.database_name]
                collection_names = await db.list_collection_names()
                
                return [{'value': coll_name, 'display_name': coll_name,
                        'description': f'é›†åˆ: {coll_name}'} for coll_name in collection_names]
            except Exception:
                return []
        
        # è®¾ç½®éªŒè¯è§„åˆ™
        validator.add_required_parameter(
            name="instance_id",
            type_check=lambda x: is_non_empty_string(x) and is_valid_instance_id(x),
            validator=lambda x, ctx: validate_instance_exists(x, ctx.connection_manager),
            options_provider=get_instance_options,
            description="MongoDBå®ä¾‹åç§°ï¼ˆæ³¨æ„ï¼šå‚æ•°åä¸ºinstance_idä½†å®é™…ä½¿ç”¨å®ä¾‹åç§°ï¼‰",
            user_friendly_name="MongoDBå®ä¾‹"
        )
        
        validator.add_required_parameter(
            name="database_name",
            type_check=lambda x: is_non_empty_string(x) and is_valid_database_name(x),
            validator=validate_database_exists,
            options_provider=get_database_options,
            description="æ•°æ®åº“åç§°",
            user_friendly_name="æ•°æ®åº“"
        )
        
        validator.add_required_parameter(
            name="collection_name",
            type_check=lambda x: is_non_empty_string(x) and is_valid_collection_name(x),
            validator=validate_collection_exists,
            options_provider=get_collection_options,
            description="é›†åˆåç§°",
            user_friendly_name="é›†åˆ"
        )
        
        validator.add_required_parameter(
            name="query_description",
            type_check=is_non_empty_string,
            description="æŸ¥è¯¢çš„è‡ªç„¶è¯­è¨€æè¿°ï¼Œä¾‹å¦‚ï¼š'æŸ¥æ‰¾æ‰€æœ‰çŠ¶æ€ä¸ºæ¿€æ´»çš„ç”¨æˆ·'",
            user_friendly_name="æŸ¥è¯¢æè¿°"
        )
        
        validator.add_optional_parameter(
            name="query_type",
            type_check=lambda x: x in ["auto", "find", "count", "aggregate", "distinct"],
            description="æŸ¥è¯¢ç±»å‹ï¼Œautoè¡¨ç¤ºè‡ªåŠ¨è¯†åˆ«",
            user_friendly_name="æŸ¥è¯¢ç±»å‹"
        )
        
        validator.add_optional_parameter(
            name="limit",
            type_check=lambda x: is_positive_integer(x) and 1 <= x <= 1000,
            description="ç»“æœé™åˆ¶æ•°é‡ï¼ŒèŒƒå›´1-1000",
            user_friendly_name="ç»“æœæ•°é‡é™åˆ¶"
        )
        
        validator.add_optional_parameter(
            name="include_explanation",
            type_check=is_boolean,
            description="æ˜¯å¦åŒ…å«æŸ¥è¯¢è§£é‡Š",
            user_friendly_name="åŒ…å«è§£é‡Š"
        )
        
        validator.add_optional_parameter(
            name="output_format",
            type_check=lambda x: x in ["full", "query_only", "executable"],
            description="è¾“å‡ºæ ¼å¼é€‰æ‹©",
            user_friendly_name="è¾“å‡ºæ ¼å¼"
        )
        
        return validator

    @with_error_handling({"component": "query_generation", "operation": "execute"})
    @with_retry(RetryConfig(max_attempts=2, base_delay=1.0))
    async def execute(self, arguments: Dict[str, Any]) -> List[TextContent]:
        """æ‰§è¡ŒæŸ¥è¯¢ç”Ÿæˆ"""
        # å‚æ•°éªŒè¯å’Œæ™ºèƒ½è¡¥å…¨
        context = self.context_manager.get_or_create_context()
        context.connection_manager = self.connection_manager
        
        # å°è¯•ä»ä¸Šä¸‹æ–‡æ¨æ–­ç¼ºå¤±å‚æ•°
        inferred_params = context.infer_missing_parameters()
        for param_name in ["instance_id", "database_name", "collection_name"]:
            if not arguments.get(param_name) and inferred_params.get(param_name):
                arguments[param_name] = inferred_params[param_name]
                logger.info(f"ä»ä¸Šä¸‹æ–‡æ¨æ–­{param_name}", value=arguments[param_name])
        
        # æ›´æ–°ä¸Šä¸‹æ–‡ä»¥æ”¯æŒæ•°æ®åº“å’Œé›†åˆé€‰é¡¹çš„è·å–
        if arguments.get("instance_id"):
            context = context.clone_with_updates(instance_id=arguments["instance_id"])
        if arguments.get("database_name"):
            context = context.clone_with_updates(database_name=arguments["database_name"])
        
        validation_result, errors = await self.validator.validate_parameters(arguments, context)
        
        if validation_result != ValidationResult.VALID:
            return MCPParameterHelper.create_error_response(errors)
        
        # è®°å½•å·¥å…·è°ƒç”¨åˆ°ä¸Šä¸‹æ–‡å¹¶æ›´æ–°ä¸Šä¸‹æ–‡
        context.add_to_chain("generate_query", arguments)
        self.context_manager.update_context(
            instance_id=arguments["instance_id"],
            database_name=arguments["database_name"],
            collection_name=arguments["collection_name"]
        )
        
        instance_id = arguments["instance_id"]
        database_name = arguments["database_name"]
        collection_name = arguments["collection_name"]
        query_description = arguments["query_description"]
        query_type = arguments.get("query_type", "auto")
        limit = arguments.get("limit", 100)
        include_explanation = arguments.get("include_explanation", True)
        output_format = arguments.get("output_format", "full")
        
        logger.info(
            "å¼€å§‹ç”ŸæˆæŸ¥è¯¢",
            instance_id=instance_id,
            database=database_name,
            collection=collection_name,
            description=query_description
        )
        
        try:
            # éªŒè¯å®ä¾‹å’Œé›†åˆ
            validation_result = await self._validate_target(instance_id, database_name, collection_name)
            if validation_result:
                return [TextContent(type="text", text=validation_result)]
            
            # è·å–é›†åˆå­—æ®µä¿¡æ¯
            fields = await self.metadata_manager.get_fields_by_collection(
                instance_id, instance_id, database_name, collection_name
            )
            
            if not fields:
                return [TextContent(
                    type="text",
                    text=f"é›†åˆ '{database_name}.{collection_name}' æ²¡æœ‰å­—æ®µä¿¡æ¯ã€‚è¯·å…ˆä½¿ç”¨ analyze_collection å·¥å…·æ‰«æé›†åˆç»“æ„ã€‚"
                )]
            
            # åˆ†ææŸ¥è¯¢æè¿°
            query_analysis = await self._analyze_query_description(
                query_description, fields, query_type
            )
            
            # ç”ŸæˆæŸ¥è¯¢
            query_result = await self._generate_query(
                query_analysis, fields, limit
            )
            
            if "error" in query_result:
                return [TextContent(type="text", text=f"ç”ŸæˆæŸ¥è¯¢å¤±è´¥: {query_result['error']}")]
            
            # æ„å»ºç»“æœ
            result_text = await self._build_query_result(
                query_result, query_description, instance_id, database_name, 
                collection_name, include_explanation, query_analysis, output_format
            )
            
            # ä¿å­˜æŸ¥è¯¢å†å²
            await self._save_query_history(
                instance_id, database_name, collection_name,
                query_description, query_result
            )
            
            logger.info(
                "æŸ¥è¯¢ç”Ÿæˆå®Œæˆ",
                instance_id=instance_id,
                database=database_name,
                collection=collection_name,
                query_type=query_result.get("query_type")
            )
            
            return [TextContent(type="text", text=result_text)]
            
        except Exception as e:
            error_msg = f"ç”ŸæˆæŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
            logger.error(
                "æŸ¥è¯¢ç”Ÿæˆå¤±è´¥",
                instance_id=instance_id,
                database=database_name,
                collection=collection_name,
                error=str(e)
            )
            return [TextContent(type="text", text=error_msg)]
    
    @with_error_handling({"component": "query_generation", "operation": "validate_target"})
    async def _validate_target(self, instance_id: str, database_name: str, collection_name: str) -> Optional[str]:
        """éªŒè¯ç›®æ ‡å®ä¾‹å’Œé›†åˆ"""
        # éªŒè¯å®ä¾‹
        if not self.connection_manager.has_instance(instance_id):
            return f"å®ä¾‹ '{instance_id}' ä¸å­˜åœ¨ã€‚è¯·ä½¿ç”¨ discover_instances å·¥å…·æŸ¥çœ‹å¯ç”¨å®ä¾‹ã€‚"
        
        # æ£€æŸ¥å®ä¾‹å¥åº·çŠ¶æ€
        health_status = await self.connection_manager.check_instance_health(instance_id)
        if not health_status["healthy"]:
            return f"å®ä¾‹ '{instance_id}' ä¸å¥åº·: {health_status.get('error', 'Unknown error')}"
        
        # éªŒè¯é›†åˆæ˜¯å¦å­˜åœ¨
        try:
            connection = self.connection_manager.get_instance_connection(instance_id)
            if connection:
                db = connection.get_database(database_name)
                collection_names = await db.list_collection_names()
                if collection_name not in collection_names:
                    return f"é›†åˆ '{database_name}.{collection_name}' ä¸å­˜åœ¨ã€‚"
        except Exception as e:
            return f"éªŒè¯é›†åˆæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
        
        return None
    
    @with_error_handling({"component": "query_generation", "operation": "analyze_query"})
    async def _analyze_query_description(self, description: str, fields: List[Dict[str, Any]], 
                                       query_type: str) -> Dict[str, Any]:
        """åˆ†ææŸ¥è¯¢æè¿°"""
        analysis = {
            "query_type": query_type,
            "conditions": [],
            "projections": [],
            "sort_fields": [],
            "group_fields": [],
            "time_filters": [],
            "text_searches": [],
            "numeric_ranges": []
        }
        
        description_lower = description.lower()
        
        # è‡ªåŠ¨è¯†åˆ«æŸ¥è¯¢ç±»å‹
        if query_type == "auto":
            analysis["query_type"] = self._detect_query_type(description_lower)
        
        # è·å–å­—æ®µå»ºè®®
        field_suggestions = self.semantic_analyzer.get_semantic_suggestions_for_query(
            description, fields
        )
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœªçŸ¥å­—æ®µéœ€è¦è¯­ä¹‰è¡¥å…¨
        unknown_fields = self._detect_unknown_fields(description, field_suggestions)
        if unknown_fields:
            # å°è¯•è¯­ä¹‰è¡¥å…¨
            completion_result = await self._try_semantic_completion(
                description, unknown_fields, fields
            )
            if completion_result.get("suggestions"):
                # åˆå¹¶è¡¥å…¨å»ºè®®åˆ°å­—æ®µå»ºè®®ä¸­
                field_suggestions.extend(completion_result["suggestions"])
                analysis["semantic_completion"] = completion_result
        
        # åˆ†ææ¡ä»¶
        analysis["conditions"] = self._extract_conditions(
            description, field_suggestions
        )
        
        # åˆ†ææ—¶é—´è¿‡æ»¤
        analysis["time_filters"] = self._extract_time_filters(
            description, field_suggestions
        )
        
        # åˆ†ææ–‡æœ¬æœç´¢
        analysis["text_searches"] = self._extract_text_searches(
            description, field_suggestions
        )
        
        # åˆ†ææ•°å€¼èŒƒå›´
        analysis["numeric_ranges"] = self._extract_numeric_ranges(
            description, field_suggestions
        )
        
        # åˆ†ææ’åº
        analysis["sort_fields"] = self._extract_sort_fields(
            description, field_suggestions
        )
        
        # åˆ†æåˆ†ç»„ï¼ˆç”¨äºèšåˆæŸ¥è¯¢ï¼‰
        if analysis["query_type"] == "aggregate":
            analysis["group_fields"] = self._extract_group_fields(
                description, field_suggestions
            )
        
        return analysis
    
    def _detect_unknown_fields(self, description: str, field_suggestions: List[Dict[str, Any]]) -> List[str]:
        """æ£€æµ‹æŸ¥è¯¢æè¿°ä¸­çš„æœªçŸ¥å­—æ®µ"""
        import jieba
        
        # åˆ†è¯æå–å¯èƒ½çš„å­—æ®µå
        words = list(jieba.cut(description))
        
        # è¿‡æ»¤å‡ºå¯èƒ½æ˜¯å­—æ®µåçš„è¯æ±‡
        potential_fields = []
        for word in words:
            if len(word) > 1 and word.isalnum():
                potential_fields.append(word)
        
        # æ£€æŸ¥å“ªäº›å­—æ®µåœ¨ç°æœ‰å»ºè®®ä¸­æ‰¾ä¸åˆ°
        existing_fields = {suggestion.get("field_path", "").lower() 
                          for suggestion in field_suggestions}
        
        unknown_fields = []
        for field in potential_fields:
            if field.lower() not in existing_fields:
                # è¿›ä¸€æ­¥æ£€æŸ¥æ˜¯å¦çœŸçš„åƒå­—æ®µå
                if self._looks_like_field_name(field):
                    unknown_fields.append(field)
        
        return unknown_fields
    
    def _looks_like_field_name(self, word: str) -> bool:
        """åˆ¤æ–­è¯æ±‡æ˜¯å¦åƒå­—æ®µå"""
        # ç®€å•çš„å¯å‘å¼è§„åˆ™
        if len(word) < 2:
            return False
        
        # åŒ…å«å¸¸è§å­—æ®µå…³é”®è¯
        field_keywords = ["åç§°", "å§“å", "æ—¶é—´", "æ—¥æœŸ", "çŠ¶æ€", "ç±»å‹", "ç¼–å·", "ID", "id", 
                         "name", "time", "date", "status", "type", "code", "number"]
        
        for keyword in field_keywords:
            if keyword in word:
                return True
        
        # æˆ–è€…æ˜¯è‹±æ–‡å­—æ®µåæ¨¡å¼
        if word.replace("_", "").isalnum() and any(c.islower() for c in word):
            return True
            
        return False
    
    async def _try_semantic_completion(self, description: str, unknown_fields: List[str], 
                                     fields: List[Dict[str, Any]]) -> Dict[str, Any]:
        """å°è¯•è¯­ä¹‰è¡¥å…¨"""
        try:
            # è°ƒç”¨ç»Ÿä¸€è¯­ä¹‰å·¥å…·è¿›è¡Œè¯­ä¹‰å»ºè®®
            completion_args = {
                "action": "suggest_semantics",
                "instance_name": "default",  # ä½¿ç”¨é»˜è®¤å®ä¾‹
                "query_description": description,
                "unknown_fields": unknown_fields,
                "available_fields": [field.get("field_path", "") for field in fields],
                "field_types": {field.get("field_path", ""): field.get("field_type", "string") 
                               for field in fields}
            }
            
            result = await self.unified_semantic.execute(completion_args)
            
            # è§£æç»“æœ
            if result and len(result) > 0:
                content = result[0].text if hasattr(result[0], 'text') else str(result[0])
                try:
                    return json.loads(content)
                except json.JSONDecodeError:
                    return {"suggestions": [], "message": content}
            
            return {"suggestions": []}
            
        except Exception as e:
            logger.warning("è¯­ä¹‰è¡¥å…¨å¤±è´¥", error=str(e))
            return {"suggestions": [], "error": str(e)}
        
    def _detect_query_type(self, description: str) -> str:
        """æ£€æµ‹æŸ¥è¯¢ç±»å‹"""
        for pattern, query_type in self.query_patterns.items():
            if re.search(pattern, description):
                if query_type in ['find', 'count', 'aggregate', 'distinct']:
                    return query_type
        
        # é»˜è®¤ä¸ºæŸ¥æ‰¾
        return 'find'
    
    def _extract_conditions(self, description: str, field_suggestions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æå–æŸ¥è¯¢æ¡ä»¶"""
        conditions = []
        
        # ç®€å•çš„æ¡ä»¶æå–é€»è¾‘
        for field_info in field_suggestions[:5]:  # åªè€ƒè™‘å‰5ä¸ªæœ€ç›¸å…³çš„å­—æ®µ
            field_path = field_info["field_path"]
            business_meaning = field_info.get("business_meaning", "")
            
            # æ£€æŸ¥æ˜¯å¦åœ¨æè¿°ä¸­æåˆ°äº†è¿™ä¸ªå­—æ®µ
            field_mentioned = False
            for word in [field_path.lower(), business_meaning.lower()]:
                if word and word in description.lower():
                    field_mentioned = True
                    break
            
            if field_mentioned:
                # å°è¯•æå–æ¡ä»¶æ“ä½œç¬¦å’Œå€¼
                condition = self._extract_field_condition(description, field_path, field_info)
                if condition:
                    conditions.append(condition)
        
        return conditions
    
    def _extract_field_condition(self, description: str, field_path: str, 
                               field_info: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """æå–ç‰¹å®šå­—æ®µçš„æ¡ä»¶"""
        field_type = field_info.get("field_type", "string")
        
        # æŸ¥æ‰¾æ“ä½œç¬¦
        operator = "$eq"  # é»˜è®¤ç­‰äº
        
        for pattern, op in self.query_patterns.items():
            if op.startswith("$") and re.search(pattern, description):
                operator = op
                break
        
        # å°è¯•æå–å€¼
        value = self._extract_field_value(description, field_path, field_type)
        
        if value is not None:
            condition = {
                "field": field_path,
                "operator": operator,
                "value": value,
                "field_type": field_type
            }
            
            # å¤„ç†ç‰¹æ®Šæ“ä½œç¬¦
            if operator == "$range":
                # èŒƒå›´æŸ¥è¯¢éœ€è¦ä¸¤ä¸ªå€¼
                range_values = self._extract_range_values(description, field_type)
                if range_values:
                    condition["value"] = range_values
            elif operator == "$regex":
                # æ­£åˆ™è¡¨è¾¾å¼æŸ¥è¯¢
                condition["value"] = {"$regex": str(value), "$options": "i"}
            elif operator in ["$exists", "$not_exists"]:
                condition["value"] = operator == "$exists"
                condition["operator"] = "$exists"
            elif operator in ["$null", "$not_null"]:
                if operator == "$null":
                    condition["operator"] = "$eq"
                    condition["value"] = None
                else:
                    condition["operator"] = "$ne"
                    condition["value"] = None
            
            return condition
        
        return None
    
    def _extract_field_value(self, description: str, field_path: str, field_type: str) -> Any:
        """æå–å­—æ®µå€¼"""
        # è¿™é‡Œå®ç°ç®€å•çš„å€¼æå–é€»è¾‘
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯èƒ½éœ€è¦æ›´å¤æ‚çš„NLPå¤„ç†
        
        # æå–æ•°å­—
        if field_type in ["integer", "double", "long"]:
            numbers = re.findall(r'\d+(?:\.\d+)?', description)
            if numbers:
                try:
                    if field_type == "integer":
                        return int(float(numbers[0]))
                    else:
                        return float(numbers[0])
                except ValueError:
                    pass
        
        # æå–å¸ƒå°”å€¼
        if field_type == "boolean":
            if re.search(r'çœŸ|æ˜¯|true|yes|1', description.lower()):
                return True
            elif re.search(r'å‡|å¦|false|no|0', description.lower()):
                return False
        
        # æå–å­—ç¬¦ä¸²ï¼ˆç®€å•å®ç°ï¼‰
        # æŸ¥æ‰¾å¼•å·ä¸­çš„å†…å®¹
        quoted_strings = re.findall(r'["\']([^"\'\']+)["\']', description)
        if quoted_strings:
            return quoted_strings[0]
        
        # æŸ¥æ‰¾å¯èƒ½çš„å­—ç¬¦ä¸²å€¼
        words = description.split()
        for i, word in enumerate(words):
            if field_path.lower() in word.lower() and i + 1 < len(words):
                next_word = words[i + 1]
                # ç®€å•çš„å€¼æå–
                if not re.match(r'^(æ˜¯|ä¸º|ç­‰äº|å¤§äº|å°äº|åŒ…å«)$', next_word):
                    return next_word
        
        return None
    
    def _extract_range_values(self, description: str, field_type: str) -> Optional[List[Any]]:
        """æå–èŒƒå›´å€¼"""
        # æŸ¥æ‰¾ "X åˆ° Y" æˆ– "X - Y" æ¨¡å¼
        range_patterns = [
            r'(\d+(?:\.\d+)?)\s*åˆ°\s*(\d+(?:\.\d+)?)',
            r'(\d+(?:\.\d+)?)\s*-\s*(\d+(?:\.\d+)?)',
            r'(\d+(?:\.\d+)?)\s*è‡³\s*(\d+(?:\.\d+)?)',
        ]
        
        for pattern in range_patterns:
            match = re.search(pattern, description)
            if match:
                try:
                    start_val = float(match.group(1))
                    end_val = float(match.group(2))
                    
                    if field_type == "integer":
                        return [int(start_val), int(end_val)]
                    else:
                        return [start_val, end_val]
                except ValueError:
                    continue
        
        return None
    
    def _extract_time_filters(self, description: str, field_suggestions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æå–æ—¶é—´è¿‡æ»¤æ¡ä»¶"""
        time_filters = []
        
        # æŸ¥æ‰¾æ—¶é—´å­—æ®µ
        time_fields = [f for f in field_suggestions if 'time' in f["field_path"].lower() or 
                      'date' in f["field_path"].lower() or f.get("field_type") == "date"]
        
        if not time_fields:
            return time_filters
        
        # æŸ¥æ‰¾æ—¶é—´å…³é”®è¯
        for pattern, time_value in self.time_keywords.items():
            if re.search(pattern, description):
                for time_field in time_fields:
                    time_range = self._calculate_time_range(time_value)
                    if time_range:
                        time_filters.append({
                            "field": time_field["field_path"],
                            "operator": "$gte",
                            "value": time_range["start"]
                        })
                        time_filters.append({
                            "field": time_field["field_path"],
                            "operator": "$lt",
                            "value": time_range["end"]
                        })
                break
        
        return time_filters
    
    def _calculate_time_range(self, time_value: Union[int, str]) -> Optional[Dict[str, datetime]]:
        """è®¡ç®—æ—¶é—´èŒƒå›´"""
        now = datetime.now()
        
        if isinstance(time_value, int):
            # ç›¸å¯¹å¤©æ•°
            target_date = now + timedelta(days=time_value)
            return {
                "start": target_date.replace(hour=0, minute=0, second=0, microsecond=0),
                "end": target_date.replace(hour=23, minute=59, second=59, microsecond=999999)
            }
        elif time_value == "this_week":
            # æœ¬å‘¨
            start_of_week = now - timedelta(days=now.weekday())
            end_of_week = start_of_week + timedelta(days=6)
            return {
                "start": start_of_week.replace(hour=0, minute=0, second=0, microsecond=0),
                "end": end_of_week.replace(hour=23, minute=59, second=59, microsecond=999999)
            }
        elif time_value == "this_month":
            # æœ¬æœˆ
            start_of_month = now.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
            if now.month == 12:
                end_of_month = start_of_month.replace(year=now.year + 1, month=1) - timedelta(days=1)
            else:
                end_of_month = start_of_month.replace(month=now.month + 1) - timedelta(days=1)
            return {
                "start": start_of_month,
                "end": end_of_month.replace(hour=23, minute=59, second=59, microsecond=999999)
            }
        
        return None
    
    def _extract_text_searches(self, description: str, field_suggestions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æå–æ–‡æœ¬æœç´¢æ¡ä»¶"""
        text_searches = []
        
        # æŸ¥æ‰¾æ–‡æœ¬å­—æ®µ
        text_fields = [f for f in field_suggestions if f.get("field_type") == "string"]
        
        # æŸ¥æ‰¾åŒ…å«å…³é”®è¯
        if re.search(r'åŒ…å«|å«æœ‰', description):
            # æå–è¦æœç´¢çš„æ–‡æœ¬
            search_terms = re.findall(r'åŒ…å«["\']([^"\'\']+)["\']', description)
            if not search_terms:
                search_terms = re.findall(r'å«æœ‰["\']([^"\'\']+)["\']', description)
            
            for term in search_terms:
                for field in text_fields[:3]:  # é™åˆ¶å­—æ®µæ•°é‡
                    text_searches.append({
                        "field": field["field_path"],
                        "operator": "$regex",
                        "value": {"$regex": term, "$options": "i"}
                    })
        
        return text_searches
    
    def _extract_numeric_ranges(self, description: str, field_suggestions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æå–æ•°å€¼èŒƒå›´æ¡ä»¶"""
        numeric_ranges = []
        
        # æŸ¥æ‰¾æ•°å€¼å­—æ®µ
        numeric_fields = [f for f in field_suggestions if 
                         f.get("field_type") in ["integer", "double", "long"]]
        
        # æŸ¥æ‰¾èŒƒå›´è¡¨è¾¾å¼
        range_patterns = [
            r'(\w+)\s*åœ¨\s*(\d+(?:\.\d+)?)\s*åˆ°\s*(\d+(?:\.\d+)?)\s*ä¹‹é—´',
            r'(\w+)\s*èŒƒå›´\s*(\d+(?:\.\d+)?)\s*-\s*(\d+(?:\.\d+)?)',
        ]
        
        for pattern in range_patterns:
            matches = re.finditer(pattern, description)
            for match in matches:
                field_name = match.group(1)
                start_val = float(match.group(2))
                end_val = float(match.group(3))
                
                # æŸ¥æ‰¾åŒ¹é…çš„å­—æ®µ
                for field in numeric_fields:
                    if (field_name.lower() in field["field_path"].lower() or 
                        field_name.lower() in field.get("business_meaning", "").lower()):
                        
                        field_type = field.get("field_type")
                        if field_type == "integer":
                            start_val = int(start_val)
                            end_val = int(end_val)
                        
                        numeric_ranges.extend([
                            {
                                "field": field["field_path"],
                                "operator": "$gte",
                                "value": start_val
                            },
                            {
                                "field": field["field_path"],
                                "operator": "$lte",
                                "value": end_val
                            }
                        ])
                        break
        
        return numeric_ranges
    
    def _extract_sort_fields(self, description: str, field_suggestions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æå–æ’åºå­—æ®µ"""
        sort_fields = []
        
        # æŸ¥æ‰¾æ’åºå…³é”®è¯
        sort_patterns = [
            (r'æŒ‰\s*(\w+)\s*å‡åº|æŒ‰\s*(\w+)\s*æ­£åº', 1),
            (r'æŒ‰\s*(\w+)\s*é™åº|æŒ‰\s*(\w+)\s*å€’åº', -1),
            (r'(\w+)\s*ä»å°åˆ°å¤§', 1),
            (r'(\w+)\s*ä»å¤§åˆ°å°', -1),
        ]
        
        for pattern, direction in sort_patterns:
            matches = re.finditer(pattern, description)
            for match in matches:
                field_name = match.group(1) or match.group(2)
                if field_name:
                    # æŸ¥æ‰¾åŒ¹é…çš„å­—æ®µ
                    for field in field_suggestions:
                        if (field_name.lower() in field["field_path"].lower() or 
                            field_name.lower() in field.get("business_meaning", "").lower()):
                            sort_fields.append({
                                "field": field["field_path"],
                                "direction": direction
                            })
                            break
        
        return sort_fields
    
    def _extract_group_fields(self, description: str, field_suggestions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æå–åˆ†ç»„å­—æ®µ"""
        group_fields = []
        
        # æŸ¥æ‰¾åˆ†ç»„å…³é”®è¯
        group_patterns = [
            r'æŒ‰\s*(\w+)\s*åˆ†ç»„',
            r'æ ¹æ®\s*(\w+)\s*åˆ†ç»„',
            r'ä»¥\s*(\w+)\s*ä¸ºç»„',
        ]
        
        for pattern in group_patterns:
            matches = re.finditer(pattern, description)
            for match in matches:
                field_name = match.group(1)
                # æŸ¥æ‰¾åŒ¹é…çš„å­—æ®µ
                for field in field_suggestions:
                    if (field_name.lower() in field["field_path"].lower() or 
                        field_name.lower() in field.get("business_meaning", "").lower()):
                        group_fields.append({
                            "field": field["field_path"]
                        })
                        break
        
        return group_fields
    
    @with_error_handling({"component": "query_generation", "operation": "generate_query"})
    @with_retry(RetryConfig(max_attempts=2, base_delay=1.0))
    async def _generate_query(self, analysis: Dict[str, Any], fields: List[Dict[str, Any]], 
                            limit: int) -> Dict[str, Any]:
        """ç”ŸæˆæŸ¥è¯¢è¯­å¥"""
        query_type = analysis["query_type"]
        
        try:
            if query_type == "find":
                return self._generate_find_query(analysis, limit)
            elif query_type == "count":
                return self._generate_count_query(analysis)
            elif query_type == "aggregate":
                return self._generate_aggregate_query(analysis, limit)
            elif query_type == "distinct":
                return self._generate_distinct_query(analysis, fields)
            else:
                return {"error": f"ä¸æ”¯æŒçš„æŸ¥è¯¢ç±»å‹: {query_type}"}
                
        except Exception as e:
            return {"error": f"ç”ŸæˆæŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"}
    
    @with_error_handling({"component": "query_generation", "operation": "generate_find_query"})
    def _generate_find_query(self, analysis: Dict[str, Any], limit: int) -> Dict[str, Any]:
        """ç”ŸæˆæŸ¥æ‰¾æŸ¥è¯¢"""
        query = {}
        
        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        all_conditions = (analysis["conditions"] + analysis["time_filters"] + 
                         analysis["text_searches"] + analysis["numeric_ranges"])
        
        for condition in all_conditions:
            field = condition["field"]
            operator = condition["operator"]
            value = condition["value"]
            
            if operator == "$eq":
                query[field] = value
            elif operator in ["$ne", "$gt", "$gte", "$lt", "$lte"]:
                if field not in query:
                    query[field] = {}
                query[field][operator] = value
            elif operator == "$regex":
                query[field] = value
            elif operator == "$exists":
                query[field] = {"$exists": value}
        
        # æ„å»ºæ’åº
        sort = []
        for sort_field in analysis["sort_fields"]:
            sort.append((sort_field["field"], sort_field["direction"]))
        
        return {
            "query_type": "find",
            "filter": query,
            "sort": sort,
            "limit": limit,
            "mongodb_query": {
                "filter": query,
                "sort": dict(sort) if sort else None,
                "limit": limit
            }
        }
    
    @with_error_handling({"component": "query_generation", "operation": "generate_count_query"})
    def _generate_count_query(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆè®¡æ•°æŸ¥è¯¢"""
        query = {}
        
        # æ„å»ºæŸ¥è¯¢æ¡ä»¶ï¼ˆä¸findæŸ¥è¯¢ç›¸åŒï¼‰
        all_conditions = (analysis["conditions"] + analysis["time_filters"] + 
                         analysis["text_searches"] + analysis["numeric_ranges"])
        
        for condition in all_conditions:
            field = condition["field"]
            operator = condition["operator"]
            value = condition["value"]
            
            if operator == "$eq":
                query[field] = value
            elif operator in ["$ne", "$gt", "$gte", "$lt", "$lte"]:
                if field not in query:
                    query[field] = {}
                query[field][operator] = value
            elif operator == "$regex":
                query[field] = value
            elif operator == "$exists":
                query[field] = {"$exists": value}
        
        return {
            "query_type": "count",
            "filter": query,
            "mongodb_query": {
                "filter": query
            }
        }
    
    @with_error_handling({"component": "query_generation", "operation": "generate_aggregate_query"})
    def _generate_aggregate_query(self, analysis: Dict[str, Any], limit: int) -> Dict[str, Any]:
        """ç”ŸæˆèšåˆæŸ¥è¯¢"""
        pipeline = []
        
        # æ·»åŠ åŒ¹é…é˜¶æ®µ
        match_conditions = {}
        all_conditions = (analysis["conditions"] + analysis["time_filters"] + 
                         analysis["text_searches"] + analysis["numeric_ranges"])
        
        for condition in all_conditions:
            field = condition["field"]
            operator = condition["operator"]
            value = condition["value"]
            
            if operator == "$eq":
                match_conditions[field] = value
            elif operator in ["$ne", "$gt", "$gte", "$lt", "$lte"]:
                if field not in match_conditions:
                    match_conditions[field] = {}
                match_conditions[field][operator] = value
            elif operator == "$regex":
                match_conditions[field] = value
            elif operator == "$exists":
                match_conditions[field] = {"$exists": value}
        
        if match_conditions:
            pipeline.append({"$match": match_conditions})
        
        # æ·»åŠ åˆ†ç»„é˜¶æ®µ
        if analysis["group_fields"]:
            group_stage = {
                "_id": {}
            }
            
            for group_field in analysis["group_fields"]:
                field_name = group_field["field"].replace(".", "_")
                group_stage["_id"][field_name] = f"${group_field['field']}"
            
            # æ·»åŠ è®¡æ•°
            group_stage["count"] = {"$sum": 1}
            
            pipeline.append({"$group": group_stage})
        
        # æ·»åŠ æ’åºé˜¶æ®µ
        if analysis["sort_fields"]:
            sort_stage = {}
            for sort_field in analysis["sort_fields"]:
                sort_stage[sort_field["field"]] = sort_field["direction"]
            pipeline.append({"$sort": sort_stage})
        
        # æ·»åŠ é™åˆ¶é˜¶æ®µ
        pipeline.append({"$limit": limit})
        
        return {
            "query_type": "aggregate",
            "pipeline": pipeline,
            "mongodb_query": {
                "pipeline": pipeline
            }
        }
    
    @with_error_handling({"component": "query_generation", "operation": "generate_distinct_query"})
    def _generate_distinct_query(self, analysis: Dict[str, Any], fields: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆå»é‡æŸ¥è¯¢"""
        # é€‰æ‹©ç¬¬ä¸€ä¸ªç›¸å…³å­—æ®µä½œä¸ºå»é‡å­—æ®µ
        distinct_field = None
        
        if analysis["conditions"]:
            distinct_field = analysis["conditions"][0]["field"]
        elif fields:
            # é€‰æ‹©ç¬¬ä¸€ä¸ªå­—æ®µ
            distinct_field = fields[0]["field_path"]
        
        if not distinct_field:
            return {"error": "æ— æ³•ç¡®å®šå»é‡å­—æ®µ"}
        
        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        query = {}
        all_conditions = (analysis["conditions"] + analysis["time_filters"] + 
                         analysis["text_searches"] + analysis["numeric_ranges"])
        
        for condition in all_conditions:
            field = condition["field"]
            operator = condition["operator"]
            value = condition["value"]
            
            if operator == "$eq":
                query[field] = value
            elif operator in ["$ne", "$gt", "$gte", "$lt", "$lte"]:
                if field not in query:
                    query[field] = {}
                query[field][operator] = value
            elif operator == "$regex":
                query[field] = value
            elif operator == "$exists":
                query[field] = {"$exists": value}
        
        return {
            "query_type": "distinct",
            "field": distinct_field,
            "filter": query,
            "mongodb_query": {
                "field": distinct_field,
                "filter": query
            }
        }
    
    @with_error_handling({"component": "query_generation", "operation": "build_query_result"})
    async def _build_query_result(self, query_result: Dict[str, Any], query_description: str,
                                instance_id: str, database_name: str, collection_name: str,
                                include_explanation: bool, analysis: Dict[str, Any] = None, 
                                output_format: str = "full") -> str:
        """æ„å»ºæŸ¥è¯¢ç»“æœæ–‡æœ¬"""
        query_type = query_result["query_type"]
        mongodb_query = query_result["mongodb_query"]
        
        # ç”Ÿæˆå¯æ‰§è¡Œçš„MongoDBæŸ¥è¯¢è¯­å¥
        executable_query = self._generate_executable_query(query_type, collection_name, mongodb_query)
        
        # æ ¹æ®è¾“å‡ºæ ¼å¼è¿”å›ä¸åŒå†…å®¹
        if output_format == "executable":
            return executable_query
        elif output_format == "query_only":
            result_text = f"**æŸ¥è¯¢ç±»å‹**: {query_type.upper()}\n\n"
            result_text += f"```javascript\n{executable_query}\n```"
            return result_text
        
        # é»˜è®¤å®Œæ•´æ ¼å¼
        result_text = f"## æŸ¥è¯¢ç”Ÿæˆç»“æœ\n\n"
        result_text += f"**æŸ¥è¯¢æè¿°**: {query_description}\n\n"
        
        # è¯­ä¹‰è¡¥å…¨ä¿¡æ¯
        if analysis and "semantic_completion" in analysis:
            completion_info = analysis["semantic_completion"]
            if completion_info.get("suggestions"):
                result_text += f"### ğŸ” æ™ºèƒ½å­—æ®µåŒ¹é…\n\n"
                result_text += f"ç³»ç»Ÿè‡ªåŠ¨è¯†åˆ«å¹¶åŒ¹é…äº†ä»¥ä¸‹å­—æ®µï¼š\n\n"
                for suggestion in completion_info["suggestions"]:
                    field_path = suggestion.get("field_path", "")
                    confidence = suggestion.get("confidence", 0)
                    reason = suggestion.get("reason", "")
                    result_text += f"- **{field_path}** (ç½®ä¿¡åº¦: {confidence:.2f}) - {reason}\n"
                result_text += "\n"
        
        result_text += f"### ç”Ÿæˆçš„MongoDBæŸ¥è¯¢\n\n"
        result_text += f"**æŸ¥è¯¢ç±»å‹**: {query_type.upper()}\n\n"
        
        # æ˜¾ç¤ºå¯æ‰§è¡Œçš„MongoDBæŸ¥è¯¢è¯­å¥
        result_text += f"```javascript\n{executable_query}\n```\n\n"
        
        # æŸ¥è¯¢è§£é‡Š
        if include_explanation:
            result_text += await self._generate_query_explanation(query_result, query_description)
        
        # ä½¿ç”¨å»ºè®®
        result_text += f"### ä½¿ç”¨å»ºè®®\n\n"
        result_text += f"- å¯ç›´æ¥å¤åˆ¶ä¸Šè¿°æŸ¥è¯¢è¯­å¥åˆ°MongoDB shellæˆ–å®¢æˆ·ç«¯ä¸­æ‰§è¡Œ\n"
        result_text += f"- ä½¿ç”¨ `confirm_query` å·¥å…·åœ¨ç³»ç»Ÿä¸­æ‰§è¡Œæ­¤æŸ¥è¯¢å¹¶æŸ¥çœ‹ç»“æœ\n"
        result_text += f"- å¦‚æœæŸ¥è¯¢ç»“æœä¸ç¬¦åˆé¢„æœŸï¼Œå¯ä»¥è°ƒæ•´æŸ¥è¯¢æè¿°é‡æ–°ç”Ÿæˆ\n"
        result_text += f"- å¯¹äºå¤§æ•°æ®é›†ï¼Œå»ºè®®å…ˆä½¿ç”¨ count æŸ¥è¯¢ç¡®è®¤ç»“æœæ•°é‡\n"
        
        return result_text
    
    def _generate_executable_query(self, query_type: str, collection_name: str, mongodb_query: Dict[str, Any]) -> str:
        """ç”Ÿæˆå¯ç›´æ¥æ‰§è¡Œçš„MongoDBæŸ¥è¯¢è¯­å¥"""
        if query_type == "find":
            filter_query = mongodb_query.get("filter", {})
            sort_query = mongodb_query.get("sort")
            limit_query = mongodb_query.get("limit")
            
            query_str = f"db.{collection_name}.find("
            query_str += json.dumps(filter_query, ensure_ascii=False, default=str, separators=(',', ':'))
            query_str += ")"
            
            if sort_query:
                query_str += f".sort({json.dumps(sort_query, ensure_ascii=False, separators=(',', ':'))})"
            
            if limit_query:
                query_str += f".limit({limit_query})"
            
            return query_str
            
        elif query_type == "count":
            filter_query = mongodb_query.get("filter", {})
            query_str = f"db.{collection_name}.countDocuments("
            query_str += json.dumps(filter_query, ensure_ascii=False, default=str, separators=(',', ':'))
            query_str += ")"
            return query_str
            
        elif query_type == "aggregate":
            pipeline = mongodb_query.get("pipeline", [])
            query_str = f"db.{collection_name}.aggregate("
            query_str += json.dumps(pipeline, ensure_ascii=False, default=str, separators=(',', ':'))
            query_str += ")"
            return query_str
            
        elif query_type == "distinct":
            field = mongodb_query.get("field")
            filter_query = mongodb_query.get("filter", {})
            query_str = f"db.{collection_name}.distinct("
            query_str += f'"{field}", '
            query_str += json.dumps(filter_query, ensure_ascii=False, default=str, separators=(',', ':'))
            query_str += ")"
            return query_str
        
        return f"// ä¸æ”¯æŒçš„æŸ¥è¯¢ç±»å‹: {query_type}"
    
    async def _generate_query_explanation(self, query_result: Dict[str, Any], query_description: str) -> str:
        """ç”ŸæˆæŸ¥è¯¢è§£é‡Š"""
        explanation = f"### æŸ¥è¯¢è§£é‡Š\n\n"
        
        query_type = query_result["query_type"]
        
        if query_type == "find":
            filter_query = query_result.get("filter", {})
            sort_query = query_result.get("sort", [])
            limit_query = query_result.get("limit")
            
            explanation += f"æ­¤æŸ¥è¯¢å°†ï¼š\n"
            
            if filter_query:
                explanation += f"1. **ç­›é€‰æ¡ä»¶**: æ ¹æ®ä»¥ä¸‹æ¡ä»¶ç­›é€‰æ–‡æ¡£ï¼š\n"
                for field, condition in filter_query.items():
                    if isinstance(condition, dict):
                        for op, value in condition.items():
                            op_desc = self._get_operator_description(op)
                            explanation += f"   - {field} {op_desc} {value}\n"
                    else:
                        explanation += f"   - {field} ç­‰äº {condition}\n"
            else:
                explanation += f"1. **ç­›é€‰æ¡ä»¶**: æ— ç­›é€‰æ¡ä»¶ï¼Œè¿”å›æ‰€æœ‰æ–‡æ¡£\n"
            
            if sort_query:
                explanation += f"2. **æ’åº**: æŒ‰ä»¥ä¸‹å­—æ®µæ’åºï¼š\n"
                for field, direction in sort_query:
                    direction_desc = "å‡åº" if direction == 1 else "é™åº"
                    explanation += f"   - {field} ({direction_desc})\n"
            
            if limit_query:
                explanation += f"3. **é™åˆ¶**: æœ€å¤šè¿”å› {limit_query} æ¡è®°å½•\n"
            
        elif query_type == "count":
            explanation += f"æ­¤æŸ¥è¯¢å°†ç»Ÿè®¡æ»¡è¶³æ¡ä»¶çš„æ–‡æ¡£æ•°é‡\n"
            
        elif query_type == "aggregate":
            explanation += f"æ­¤æŸ¥è¯¢ä½¿ç”¨èšåˆç®¡é“è¿›è¡Œå¤æ‚æ•°æ®å¤„ç†\n"
            
        elif query_type == "distinct":
            field = query_result.get("field")
            explanation += f"æ­¤æŸ¥è¯¢å°†è¿”å›å­—æ®µ '{field}' çš„æ‰€æœ‰å”¯ä¸€å€¼\n"
        
        explanation += "\n"
        return explanation
    
    def _get_operator_description(self, operator: str) -> str:
        """è·å–æ“ä½œç¬¦æè¿°"""
        descriptions = {
            "$eq": "ç­‰äº",
            "$ne": "ä¸ç­‰äº",
            "$gt": "å¤§äº",
            "$gte": "å¤§äºç­‰äº",
            "$lt": "å°äº",
            "$lte": "å°äºç­‰äº",
            "$regex": "åŒ¹é…æ­£åˆ™è¡¨è¾¾å¼",
            "$exists": "å­˜åœ¨" if operator == "$exists" else "ä¸å­˜åœ¨"
        }
        return descriptions.get(operator, operator)
    
    @with_error_handling({"component": "query_generation", "operation": "save_query_history"})
    async def _save_query_history(self, instance_id: str, database_name: str, collection_name: str,
                                query_description: str, query_result: Dict[str, Any]):
        """ä¿å­˜æŸ¥è¯¢å†å²"""
        try:
            await self.metadata_manager.save_query_history(
                instance_id=instance_id,
                database_name=database_name,
                collection_name=collection_name,
                query_description=query_description,
                query_type=query_result["query_type"],
                mongodb_query=query_result["mongodb_query"],
                generated_at=datetime.now()
            )
        except Exception as e:
            logger.warning("ä¿å­˜æŸ¥è¯¢å†å²å¤±è´¥", error=str(e))