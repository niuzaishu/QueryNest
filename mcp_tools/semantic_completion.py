# -*- coding: utf-8 -*-
"""è¯­ä¹‰è¡¥å…¨å·¥å…·

å½“å…ƒæ•°æ®åº“æ‰¾ä¸åˆ°è¯­ä¹‰æ—¶ï¼Œå°è¯•è§£è¯»å…¶ä»–åº“è¡¨è¿›è¡Œè¯­ä¹‰è¡¥å…¨å’Œç”¨æˆ·ç¡®è®¤
"""

from typing import Dict, List, Any, Optional, Tuple
import structlog
from mcp.types import Tool, TextContent
import re
from datetime import datetime

from database.connection_manager import ConnectionManager
from database.metadata_manager import MetadataManager
from scanner.semantic_analyzer import SemanticAnalyzer


logger = structlog.get_logger(__name__)


class SemanticCompletionTool:
    """è¯­ä¹‰è¡¥å…¨å·¥å…·"""
    
    def __init__(self, connection_manager: ConnectionManager, 
                 metadata_manager: MetadataManager,
                 semantic_analyzer: SemanticAnalyzer):
        self.connection_manager = connection_manager
        self.metadata_manager = metadata_manager
        self.semantic_analyzer = semantic_analyzer
    
    def get_tool_definition(self) -> Tool:
        """è·å–å·¥å…·å®šä¹‰"""
        return Tool(
            name="semantic_completion",
            description="å½“å…ƒæ•°æ®åº“æ‰¾ä¸åˆ°è¯­ä¹‰æ—¶ï¼Œå°è¯•è§£è¯»å…¶ä»–åº“è¡¨è¿›è¡Œè¯­ä¹‰è¡¥å…¨å’Œç”¨æˆ·ç¡®è®¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "action": {
                        "type": "string",
                        "enum": ["analyze_unknown_field", "cross_reference_analysis", "suggest_semantics", "confirm_semantics"],
                        "description": "æ“ä½œç±»å‹ï¼šanalyze_unknown_field=åˆ†ææœªçŸ¥å­—æ®µï¼Œcross_reference_analysis=è·¨åº“è¡¨åˆ†æï¼Œsuggest_semantics=è¯­ä¹‰å»ºè®®ï¼Œconfirm_semantics=ç¡®è®¤è¯­ä¹‰"
                    },
                    "instance_name": {
                        "type": "string",
                        "description": "å®ä¾‹åç§°"
                    },
                    "database_name": {
                        "type": "string",
                        "description": "æ•°æ®åº“åç§°"
                    },
                    "collection_name": {
                        "type": "string",
                        "description": "é›†åˆåç§°"
                    },
                    "field_path": {
                        "type": "string",
                        "description": "å­—æ®µè·¯å¾„ï¼ˆç”¨äºanalyze_unknown_fieldå’Œconfirm_semanticsï¼‰"
                    },
                    "query_description": {
                        "type": "string",
                        "description": "ç”¨æˆ·æŸ¥è¯¢æè¿°ï¼ˆç”¨äºcross_reference_analysisï¼‰"
                    },
                    "suggested_meaning": {
                        "type": "string",
                        "description": "å»ºè®®çš„è¯­ä¹‰å«ä¹‰ï¼ˆç”¨äºconfirm_semanticsï¼‰"
                    },
                    "confidence_threshold": {
                        "type": "number",
                        "default": 0.6,
                        "description": "ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„å»ºè®®éœ€è¦ç”¨æˆ·ç¡®è®¤"
                    }
                },
                "required": ["action", "instance_name"]
            }
        )
    
    async def execute(self, arguments: Dict[str, Any]) -> List[TextContent]:
        """æ‰§è¡Œè¯­ä¹‰è¡¥å…¨æ“ä½œ"""
        try:
            action = arguments["action"]
            instance_name = arguments["instance_name"]
            
            # ç¡®ä¿å®ä¾‹å…ƒæ•°æ®å·²åˆå§‹åŒ–
            if not await self.metadata_manager.init_instance_metadata(instance_name):
                return [TextContent(
                    type="text",
                    text=f"æ— æ³•åˆå§‹åŒ–å®ä¾‹ '{instance_name}' çš„å…ƒæ•°æ®åº“"
                )]
            
            if action == "analyze_unknown_field":
                return await self._analyze_unknown_field(arguments)
            elif action == "cross_reference_analysis":
                return await self._cross_reference_analysis(arguments)
            elif action == "suggest_semantics":
                return await self._suggest_semantics(arguments)
            elif action == "confirm_semantics":
                return await self._confirm_semantics(arguments)
            else:
                return [TextContent(
                    type="text",
                    text=f"ä¸æ”¯æŒçš„æ“ä½œç±»å‹: {action}"
                )]
                
        except Exception as e:
            logger.error("è¯­ä¹‰è¡¥å…¨æ“ä½œå¤±è´¥", error=str(e))
            return [TextContent(
                type="text",
                text=f"è¯­ä¹‰è¡¥å…¨æ“ä½œå¤±è´¥: {str(e)}"
            )]
    
    async def _analyze_unknown_field(self, arguments: Dict[str, Any]) -> List[TextContent]:
        """åˆ†ææœªçŸ¥å­—æ®µçš„è¯­ä¹‰"""
        instance_name = arguments["instance_name"]
        database_name = arguments["database_name"]
        collection_name = arguments["collection_name"]
        field_path = arguments["field_path"]
        confidence_threshold = arguments.get("confidence_threshold", 0.6)
        
        try:
            # è·å–å®ä¾‹ä¿¡æ¯
            instance_info = await self.metadata_manager.get_instance_by_name(instance_name, instance_name)
            if not instance_info:
                return [TextContent(
                    type="text",
                    text=f"å®ä¾‹ '{instance_name}' ä¸å­˜åœ¨"
                )]
            
            instance_id = instance_info["_id"]
            
            # è·å–å­—æ®µä¿¡æ¯
            fields = await self.metadata_manager.get_fields_by_collection(
                instance_name, instance_id, database_name, collection_name
            )
            
            target_field = None
            for field in fields:
                if field["field_path"] == field_path:
                    target_field = field
                    break
            
            if not target_field:
                return [TextContent(
                    type="text",
                    text=f"å­—æ®µ '{field_path}' åœ¨é›†åˆ '{database_name}.{collection_name}' ä¸­ä¸å­˜åœ¨"
                )]
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰è¯­ä¹‰å®šä¹‰
            if target_field.get("business_meaning"):
                return [TextContent(
                    type="text",
                    text=f"å­—æ®µ '{field_path}' å·²æœ‰è¯­ä¹‰å®šä¹‰: {target_field['business_meaning']}"
                )]
            
            result_text = f"## æœªçŸ¥å­—æ®µè¯­ä¹‰åˆ†æ: {field_path}\n\n"
            
            # 1. åŸºç¡€è¯­ä¹‰åˆ†æ
            analysis = await self.semantic_analyzer.analyze_field_semantics(
                instance_name, database_name, collection_name, field_path, target_field
            )
            
            result_text += f"### åŸºç¡€åˆ†æç»“æœ\n\n"
            result_text += f"- **å»ºè®®å«ä¹‰**: {analysis['suggested_meaning']}\n"
            result_text += f"- **ç½®ä¿¡åº¦**: {analysis['confidence']:.1%}\n"
            result_text += f"- **åˆ†æä¾æ®**: {', '.join(analysis['reasoning'])}\n\n"
            
            # 2. è·¨åº“è¡¨ç›¸ä¼¼å­—æ®µåˆ†æ
            similar_fields = await self._find_similar_fields_across_collections(
                instance_name, instance_id, field_path, database_name, collection_name
            )
            
            if similar_fields:
                result_text += f"### è·¨åº“è¡¨ç›¸ä¼¼å­—æ®µ\n\n"
                for similar in similar_fields[:5]:  # æ˜¾ç¤ºå‰5ä¸ªæœ€ç›¸ä¼¼çš„
                    similarity = similar['similarity']
                    db_name = similar['database_name']
                    coll_name = similar['collection_name']
                    similar_path = similar['field_path']
                    meaning = similar.get('business_meaning', 'æœªå®šä¹‰')
                    
                    result_text += f"- **{db_name}.{coll_name}.{similar_path}** (ç›¸ä¼¼åº¦: {similarity:.1%})\n"
                    result_text += f"  è¯­ä¹‰: {meaning}\n\n"
            
            # 3. å€¼æ¨¡å¼åˆ†æ
            value_patterns = await self._analyze_value_patterns(
                instance_name, instance_id, target_field
            )
            
            if value_patterns:
                result_text += f"### å€¼æ¨¡å¼åˆ†æ\n\n"
                for pattern in value_patterns:
                    result_text += f"- **{pattern['pattern']}**: {pattern['meaning']} (åŒ¹é…åº¦: {pattern['match_rate']:.1%})\n"
                result_text += "\n"
            
            # 4. ç”Ÿæˆæœ€ç»ˆå»ºè®®
            final_suggestions = await self._generate_final_suggestions(
                analysis, similar_fields, value_patterns, confidence_threshold
            )
            
            result_text += f"### è¯­ä¹‰å»ºè®®\n\n"
            
            if analysis['confidence'] >= confidence_threshold:
                result_text += f"âœ… **é«˜ç½®ä¿¡åº¦å»ºè®®**: {analysis['suggested_meaning']}\n\n"
                result_text += f"å»ºè®®ç›´æ¥é‡‡ç”¨æ­¤è¯­ä¹‰å®šä¹‰ã€‚\n\n"
            else:
                result_text += f"âš ï¸ **éœ€è¦ç¡®è®¤çš„å»ºè®®**:\n\n"
                for i, suggestion in enumerate(final_suggestions, 1):
                    result_text += f"{i}. **{suggestion['meaning']}** (ç½®ä¿¡åº¦: {suggestion['confidence']:.1%})\n"
                    result_text += f"   ä¾æ®: {suggestion['reasoning']}\n\n"
                
                result_text += f"ğŸ’¡ **ä¸‹ä¸€æ­¥æ“ä½œ**:\n"
                result_text += f"ä½¿ç”¨ `semantic_completion` å·¥å…·çš„ `confirm_semantics` æ“ä½œç¡®è®¤è¯­ä¹‰å®šä¹‰ã€‚\n\n"
            
            return [TextContent(type="text", text=result_text)]
            
        except Exception as e:
            logger.error("åˆ†ææœªçŸ¥å­—æ®µå¤±è´¥", error=str(e))
            return [TextContent(
                type="text",
                text=f"åˆ†ææœªçŸ¥å­—æ®µæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
            )]
    
    async def _cross_reference_analysis(self, arguments: Dict[str, Any]) -> List[TextContent]:
        """è·¨åº“è¡¨åˆ†æï¼Œæ ¹æ®æŸ¥è¯¢æè¿°æ¨æ–­å¯èƒ½çš„å­—æ®µè¯­ä¹‰"""
        instance_name = arguments["instance_name"]
        database_name = arguments["database_name"]
        collection_name = arguments["collection_name"]
        query_description = arguments["query_description"]
        
        try:
            # è·å–å®ä¾‹ä¿¡æ¯
            instance_info = await self.metadata_manager.get_instance_by_name(instance_name, instance_name)
            if not instance_info:
                return [TextContent(
                    type="text",
                    text=f"å®ä¾‹ '{instance_name}' ä¸å­˜åœ¨"
                )]
            
            instance_id = instance_info["_id"]
            
            result_text = f"## è·¨åº“è¡¨è¯­ä¹‰åˆ†æ\n\n"
            result_text += f"**æŸ¥è¯¢æè¿°**: {query_description}\n\n"
            
            # 1. æå–æŸ¥è¯¢ä¸­çš„å…³é”®è¯
            keywords = self._extract_query_keywords(query_description)
            result_text += f"### æå–çš„å…³é”®è¯\n\n"
            result_text += f"{', '.join(keywords)}\n\n"
            
            # 2. åœ¨å½“å‰é›†åˆä¸­æŸ¥æ‰¾ç›¸å…³å­—æ®µ
            current_fields = await self.metadata_manager.get_fields_by_collection(
                instance_name, instance_id, database_name, collection_name
            )
            
            relevant_fields = self._match_fields_to_keywords(current_fields, keywords)
            
            if relevant_fields:
                result_text += f"### å½“å‰é›†åˆç›¸å…³å­—æ®µ\n\n"
                for field in relevant_fields:
                    field_path = field['field_path']
                    meaning = field.get('business_meaning', 'æœªå®šä¹‰')
                    relevance = field['relevance_score']
                    
                    result_text += f"- **{field_path}** (ç›¸å…³åº¦: {relevance:.1%})\n"
                    result_text += f"  å½“å‰è¯­ä¹‰: {meaning}\n\n"
            
            # 3. è·¨åº“è¡¨æŸ¥æ‰¾ç›¸ä¼¼è¯­ä¹‰
            cross_references = await self._find_cross_references(
                instance_name, instance_id, keywords, database_name, collection_name
            )
            
            if cross_references:
                result_text += f"### è·¨åº“è¡¨å‚è€ƒ\n\n"
                for ref in cross_references[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
                    db_name = ref['database_name']
                    coll_name = ref['collection_name']
                    field_path = ref['field_path']
                    meaning = ref['business_meaning']
                    relevance = ref['relevance_score']
                    
                    result_text += f"- **{db_name}.{coll_name}.{field_path}** (ç›¸å…³åº¦: {relevance:.1%})\n"
                    result_text += f"  è¯­ä¹‰: {meaning}\n\n"
            
            # 4. ç”Ÿæˆè¯­ä¹‰è¡¥å…¨å»ºè®®
            completion_suggestions = await self._generate_completion_suggestions(
                relevant_fields, cross_references, keywords
            )
            
            if completion_suggestions:
                result_text += f"### è¯­ä¹‰è¡¥å…¨å»ºè®®\n\n"
                for i, suggestion in enumerate(completion_suggestions, 1):
                    field_path = suggestion['field_path']
                    suggested_meaning = suggestion['suggested_meaning']
                    confidence = suggestion['confidence']
                    reasoning = suggestion['reasoning']
                    
                    result_text += f"{i}. **å­—æ®µ**: {field_path}\n"
                    result_text += f"   **å»ºè®®è¯­ä¹‰**: {suggested_meaning}\n"
                    result_text += f"   **ç½®ä¿¡åº¦**: {confidence:.1%}\n"
                    result_text += f"   **ä¾æ®**: {reasoning}\n\n"
                
                result_text += f"ğŸ’¡ **ä¸‹ä¸€æ­¥æ“ä½œ**:\n"
                result_text += f"ä½¿ç”¨ `semantic_completion` å·¥å…·çš„ `confirm_semantics` æ“ä½œç¡®è®¤è¿™äº›è¯­ä¹‰å®šä¹‰ã€‚\n\n"
            else:
                result_text += f"### æœªæ‰¾åˆ°ç›¸å…³çš„è¯­ä¹‰å‚è€ƒ\n\n"
                result_text += f"å»ºè®®ï¼š\n"
                result_text += f"1. æ£€æŸ¥æŸ¥è¯¢æè¿°æ˜¯å¦å‡†ç¡®\n"
                result_text += f"2. å°è¯•ä½¿ç”¨æ›´å…·ä½“çš„å…³é”®è¯\n"
                result_text += f"3. æ‰‹åŠ¨å®šä¹‰å­—æ®µè¯­ä¹‰\n\n"
            
            return [TextContent(type="text", text=result_text)]
            
        except Exception as e:
            logger.error("è·¨åº“è¡¨åˆ†æå¤±è´¥", error=str(e))
            return [TextContent(
                type="text",
                text=f"è·¨åº“è¡¨åˆ†ææ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
            )]
    
    async def _suggest_semantics(self, arguments: Dict[str, Any]) -> List[TextContent]:
        """ä¸ºæŒ‡å®šé›†åˆçš„æ‰€æœ‰æœªå®šä¹‰è¯­ä¹‰å­—æ®µæä¾›å»ºè®®"""
        instance_name = arguments["instance_name"]
        database_name = arguments["database_name"]
        collection_name = arguments["collection_name"]
        confidence_threshold = arguments.get("confidence_threshold", 0.6)
        
        try:
            # è·å–å®ä¾‹ä¿¡æ¯
            instance_info = await self.metadata_manager.get_instance_by_name(instance_name, instance_name)
            if not instance_info:
                return [TextContent(
                    type="text",
                    text=f"å®ä¾‹ '{instance_name}' ä¸å­˜åœ¨"
                )]
            
            instance_id = instance_info["_id"]
            
            # è·å–é›†åˆä¸­çš„æ‰€æœ‰å­—æ®µ
            fields = await self.metadata_manager.get_fields_by_collection(
                instance_name, instance_id, database_name, collection_name
            )
            
            # ç­›é€‰å‡ºæœªå®šä¹‰è¯­ä¹‰çš„å­—æ®µ
            undefined_fields = [f for f in fields if not f.get("business_meaning")]
            
            if not undefined_fields:
                return [TextContent(
                    type="text",
                    text=f"é›†åˆ '{database_name}.{collection_name}' ä¸­çš„æ‰€æœ‰å­—æ®µéƒ½å·²å®šä¹‰è¯­ä¹‰"
                )]
            
            result_text = f"## è¯­ä¹‰å»ºè®®: {database_name}.{collection_name}\n\n"
            result_text += f"å‘ç° {len(undefined_fields)} ä¸ªæœªå®šä¹‰è¯­ä¹‰çš„å­—æ®µ\n\n"
            
            high_confidence_suggestions = []
            low_confidence_suggestions = []
            
            for field in undefined_fields:
                field_path = field["field_path"]
                
                # åˆ†æå­—æ®µè¯­ä¹‰
                analysis = await self.semantic_analyzer.analyze_field_semantics(
                    instance_name, database_name, collection_name, field_path, field
                )
                
                suggestion = {
                    "field_path": field_path,
                    "suggested_meaning": analysis["suggested_meaning"],
                    "confidence": analysis["confidence"],
                    "reasoning": analysis["reasoning"]
                }
                
                if analysis["confidence"] >= confidence_threshold:
                    high_confidence_suggestions.append(suggestion)
                else:
                    low_confidence_suggestions.append(suggestion)
            
            # æ˜¾ç¤ºé«˜ç½®ä¿¡åº¦å»ºè®®
            if high_confidence_suggestions:
                result_text += f"### ğŸ¯ é«˜ç½®ä¿¡åº¦å»ºè®® (â‰¥{confidence_threshold:.0%})\n\n"
                for suggestion in high_confidence_suggestions:
                    field_path = suggestion["field_path"]
                    meaning = suggestion["suggested_meaning"]
                    confidence = suggestion["confidence"]
                    
                    result_text += f"- **{field_path}**: {meaning} (ç½®ä¿¡åº¦: {confidence:.1%})\n"
                
                result_text += f"\nè¿™äº›å»ºè®®å¯ä»¥ç›´æ¥é‡‡ç”¨ã€‚\n\n"
            
            # æ˜¾ç¤ºä½ç½®ä¿¡åº¦å»ºè®®
            if low_confidence_suggestions:
                result_text += f"### âš ï¸ éœ€è¦ç¡®è®¤çš„å»ºè®® (<{confidence_threshold:.0%})\n\n"
                for suggestion in low_confidence_suggestions:
                    field_path = suggestion["field_path"]
                    meaning = suggestion["suggested_meaning"]
                    confidence = suggestion["confidence"]
                    reasoning = ", ".join(suggestion["reasoning"])
                    
                    result_text += f"- **{field_path}**: {meaning}\n"
                    result_text += f"  ç½®ä¿¡åº¦: {confidence:.1%} | ä¾æ®: {reasoning}\n\n"
            
            # æ“ä½œå»ºè®®
            result_text += f"### ğŸ’¡ æ“ä½œå»ºè®®\n\n"
            
            if high_confidence_suggestions:
                result_text += f"1. **æ‰¹é‡ç¡®è®¤é«˜ç½®ä¿¡åº¦å»ºè®®**:\n"
                for suggestion in high_confidence_suggestions:
                    field_path = suggestion["field_path"]
                    meaning = suggestion["suggested_meaning"]
                    result_text += f"   - ç¡®è®¤ `{field_path}` ä¸º \"{meaning}\"\n"
                result_text += "\n"
            
            if low_confidence_suggestions:
                result_text += f"2. **é€ä¸ªç¡®è®¤ä½ç½®ä¿¡åº¦å»ºè®®**:\n"
                result_text += f"   ä½¿ç”¨ `semantic_completion` å·¥å…·çš„ `confirm_semantics` æ“ä½œ\n\n"
            
            return [TextContent(type="text", text=result_text)]
            
        except Exception as e:
            logger.error("ç”Ÿæˆè¯­ä¹‰å»ºè®®å¤±è´¥", error=str(e))
            return [TextContent(
                type="text",
                text=f"ç”Ÿæˆè¯­ä¹‰å»ºè®®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
            )]
    
    async def _confirm_semantics(self, arguments: Dict[str, Any]) -> List[TextContent]:
        """ç¡®è®¤å¹¶ä¿å­˜å­—æ®µè¯­ä¹‰"""
        instance_name = arguments["instance_name"]
        database_name = arguments["database_name"]
        collection_name = arguments["collection_name"]
        field_path = arguments["field_path"]
        suggested_meaning = arguments["suggested_meaning"]
        
        try:
            # è·å–å®ä¾‹ä¿¡æ¯
            instance_info = await self.metadata_manager.get_instance_by_name(instance_name, instance_name)
            if not instance_info:
                return [TextContent(
                    type="text",
                    text=f"å®ä¾‹ '{instance_name}' ä¸å­˜åœ¨"
                )]
            
            instance_id = instance_info["_id"]
            
            # æ›´æ–°å­—æ®µè¯­ä¹‰
            success = await self.metadata_manager.update_field_semantics(
                instance_name, instance_id, database_name, collection_name, 
                field_path, suggested_meaning
            )
            
            if success:
                result_text = f"âœ… **è¯­ä¹‰ç¡®è®¤æˆåŠŸ**\n\n"
                result_text += f"- **å­—æ®µ**: {database_name}.{collection_name}.{field_path}\n"
                result_text += f"- **è¯­ä¹‰**: {suggested_meaning}\n"
                result_text += f"- **æ›´æ–°æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
                result_text += f"è¯­ä¹‰å®šä¹‰å·²ä¿å­˜åˆ°å…ƒæ•°æ®åº“ä¸­ã€‚\n"
                
                return [TextContent(type="text", text=result_text)]
            else:
                return [TextContent(
                    type="text",
                    text=f"âŒ è¯­ä¹‰ç¡®è®¤å¤±è´¥ï¼šæ— æ³•æ›´æ–°å­—æ®µ '{field_path}' çš„è¯­ä¹‰ä¿¡æ¯"
                )]
                
        except Exception as e:
            logger.error("ç¡®è®¤è¯­ä¹‰å¤±è´¥", error=str(e))
            return [TextContent(
                type="text",
                text=f"ç¡®è®¤è¯­ä¹‰æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
            )]
    
    async def _find_similar_fields_across_collections(
        self, instance_name: str, instance_id: str, field_path: str, 
        exclude_db: str, exclude_collection: str
    ) -> List[Dict[str, Any]]:
        """åœ¨å…¶ä»–é›†åˆä¸­æŸ¥æ‰¾ç›¸ä¼¼çš„å­—æ®µ"""
        try:
            # è·å–æ‰€æœ‰æ•°æ®åº“
            databases = await self.metadata_manager.get_databases_by_instance(instance_name, instance_id)
            
            similar_fields = []
            
            for db in databases:
                db_name = db["database_name"]
                
                # è·å–æ•°æ®åº“ä¸­çš„æ‰€æœ‰é›†åˆ
                collections = await self.metadata_manager.get_collections_by_database(
                    instance_name, instance_id, db_name
                )
                
                for collection in collections:
                    collection_name = collection["collection_name"]
                    
                    # è·³è¿‡å½“å‰é›†åˆ
                    if db_name == exclude_db and collection_name == exclude_collection:
                        continue
                    
                    # è·å–é›†åˆä¸­çš„å­—æ®µ
                    fields = await self.metadata_manager.get_fields_by_collection(
                        instance_name, instance_id, db_name, collection_name
                    )
                    
                    for field in fields:
                        # è®¡ç®—å­—æ®µåç§°ç›¸ä¼¼åº¦
                        similarity = self._calculate_field_similarity(field_path, field["field_path"])
                        
                        if similarity > 0.5:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                            similar_fields.append({
                                "database_name": db_name,
                                "collection_name": collection_name,
                                "field_path": field["field_path"],
                                "business_meaning": field.get("business_meaning"),
                                "similarity": similarity,
                                "field_type": field.get("field_type")
                            })
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            similar_fields.sort(key=lambda x: x["similarity"], reverse=True)
            
            return similar_fields
            
        except Exception as e:
            logger.error("æŸ¥æ‰¾ç›¸ä¼¼å­—æ®µå¤±è´¥", error=str(e))
            return []
    
    def _calculate_field_similarity(self, field1: str, field2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªå­—æ®µåç§°çš„ç›¸ä¼¼åº¦"""
        # ç®€å•çš„å­—ç¬¦ä¸²ç›¸ä¼¼åº¦è®¡ç®—
        field1_clean = field1.lower().replace('_', '').replace('-', '')
        field2_clean = field2.lower().replace('_', '').replace('-', '')
        
        # å®Œå…¨åŒ¹é…
        if field1_clean == field2_clean:
            return 1.0
        
        # åŒ…å«å…³ç³»
        if field1_clean in field2_clean or field2_clean in field1_clean:
            return 0.8
        
        # ç¼–è¾‘è·ç¦»ç›¸ä¼¼åº¦
        max_len = max(len(field1_clean), len(field2_clean))
        if max_len == 0:
            return 0.0
        
        edit_distance = self._levenshtein_distance(field1_clean, field2_clean)
        similarity = 1.0 - (edit_distance / max_len)
        
        return max(0.0, similarity)
    
    def _levenshtein_distance(self, s1: str, s2: str) -> int:
        """è®¡ç®—ç¼–è¾‘è·ç¦»"""
        if len(s1) < len(s2):
            return self._levenshtein_distance(s2, s1)
        
        if len(s2) == 0:
            return len(s1)
        
        previous_row = list(range(len(s2) + 1))
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row
        
        return previous_row[-1]
    
    async def _analyze_value_patterns(self, instance_name: str, instance_id: str, field_info: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åˆ†æå­—æ®µå€¼çš„æ¨¡å¼"""
        patterns = []
        examples = field_info.get("examples", [])
        
        if not examples:
            return patterns
        
        # å¸¸è§æ¨¡å¼æ£€æµ‹
        pattern_checks = [
            (r'^\d{11}$', 'æ‰‹æœºå·ç '),
            (r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$', 'é‚®ç®±åœ°å€'),
            (r'^https?://', 'URLåœ°å€'),
            (r'^\d{4}-\d{2}-\d{2}', 'æ—¥æœŸæ ¼å¼'),
            (r'^[0-9a-fA-F]{24}$', 'MongoDB ObjectId'),
            (r'^\d{13}$', 'æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰'),
            (r'^\d{10}$', 'æ—¶é—´æˆ³ï¼ˆç§’ï¼‰'),
            (r'^[0-9]{6}$', 'éªŒè¯ç /é‚®æ”¿ç¼–ç '),
        ]
        
        for pattern_regex, meaning in pattern_checks:
            matches = 0
            for example in examples:
                if example is not None and re.match(pattern_regex, str(example)):
                    matches += 1
            
            if matches > 0:
                match_rate = matches / len(examples)
                patterns.append({
                    "pattern": pattern_regex,
                    "meaning": meaning,
                    "match_rate": match_rate,
                    "matches": matches,
                    "total": len(examples)
                })
        
        # æŒ‰åŒ¹é…ç‡æ’åº
        patterns.sort(key=lambda x: x["match_rate"], reverse=True)
        
        return patterns
    
    async def _generate_final_suggestions(
        self, analysis: Dict[str, Any], similar_fields: List[Dict[str, Any]], 
        value_patterns: List[Dict[str, Any]], confidence_threshold: float
    ) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæœ€ç»ˆçš„è¯­ä¹‰å»ºè®®"""
        suggestions = []
        
        # åŸºç¡€åˆ†æå»ºè®®
        if analysis["suggested_meaning"]:
            suggestions.append({
                "meaning": analysis["suggested_meaning"],
                "confidence": analysis["confidence"],
                "reasoning": "åŸºäºå­—æ®µåç§°å’Œç±»å‹åˆ†æ"
            })
        
        # ç›¸ä¼¼å­—æ®µå»ºè®®
        for similar in similar_fields[:3]:  # å–å‰3ä¸ªæœ€ç›¸ä¼¼çš„
            if similar.get("business_meaning"):
                confidence = similar["similarity"] * 0.8  # ç›¸ä¼¼åº¦æŠ˜æ‰£
                suggestions.append({
                    "meaning": similar["business_meaning"],
                    "confidence": confidence,
                    "reasoning": f"å‚è€ƒç›¸ä¼¼å­—æ®µ {similar['database_name']}.{similar['collection_name']}.{similar['field_path']}"
                })
        
        # å€¼æ¨¡å¼å»ºè®®
        for pattern in value_patterns[:2]:  # å–å‰2ä¸ªæœ€åŒ¹é…çš„æ¨¡å¼
            if pattern["match_rate"] > 0.7:  # é«˜åŒ¹é…ç‡
                confidence = pattern["match_rate"] * 0.9
                suggestions.append({
                    "meaning": pattern["meaning"],
                    "confidence": confidence,
                    "reasoning": f"åŸºäºå€¼æ¨¡å¼åˆ†æï¼ˆåŒ¹é…ç‡: {pattern['match_rate']:.1%}ï¼‰"
                })
        
        # å»é‡å¹¶æ’åº
        unique_suggestions = []
        seen_meanings = set()
        
        for suggestion in suggestions:
            meaning = suggestion["meaning"]
            if meaning not in seen_meanings:
                unique_suggestions.append(suggestion)
                seen_meanings.add(meaning)
        
        # æŒ‰ç½®ä¿¡åº¦æ’åº
        unique_suggestions.sort(key=lambda x: x["confidence"], reverse=True)
        
        return unique_suggestions[:5]  # è¿”å›å‰5ä¸ªå»ºè®®
    
    def _extract_query_keywords(self, query_description: str) -> List[str]:
        """ä»æŸ¥è¯¢æè¿°ä¸­æå–å…³é”®è¯"""
        # ç®€å•çš„å…³é”®è¯æå–
        import jieba
        
        # ä¸­æ–‡åˆ†è¯
        words = list(jieba.cut(query_description))
        
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        stop_words = {'çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œ', 'äº†', 'ç€', 'è¿‡', 'è¦', 'ä¼š', 'èƒ½', 'å¯ä»¥', 'åº”è¯¥'}
        keywords = [word.strip() for word in words if len(word.strip()) > 1 and word.strip() not in stop_words]
        
        # è‹±æ–‡å•è¯æå–
        english_words = re.findall(r'\b[a-zA-Z]+\b', query_description)
        keywords.extend([word.lower() for word in english_words if len(word) > 2])
        
        return list(set(keywords))  # å»é‡
    
    def _match_fields_to_keywords(self, fields: List[Dict[str, Any]], keywords: List[str]) -> List[Dict[str, Any]]:
        """å°†å­—æ®µä¸å…³é”®è¯åŒ¹é…"""
        relevant_fields = []
        
        for field in fields:
            field_path = field["field_path"].lower()
            business_meaning = field.get("business_meaning", "").lower()
            
            relevance_score = 0.0
            
            for keyword in keywords:
                keyword_lower = keyword.lower()
                
                # å­—æ®µååŒ¹é…
                if keyword_lower in field_path:
                    relevance_score += 0.8
                
                # è¯­ä¹‰åŒ¹é…
                if keyword_lower in business_meaning:
                    relevance_score += 1.0
                
                # éƒ¨åˆ†åŒ¹é…
                if any(keyword_lower in part for part in field_path.split('_')):
                    relevance_score += 0.5
            
            if relevance_score > 0:
                field_copy = field.copy()
                field_copy["relevance_score"] = min(relevance_score, 1.0)
                relevant_fields.append(field_copy)
        
        # æŒ‰ç›¸å…³åº¦æ’åº
        relevant_fields.sort(key=lambda x: x["relevance_score"], reverse=True)
        
        return relevant_fields
    
    async def _find_cross_references(
        self, instance_name: str, instance_id: str, keywords: List[str], 
        exclude_db: str, exclude_collection: str
    ) -> List[Dict[str, Any]]:
        """æŸ¥æ‰¾è·¨åº“è¡¨çš„è¯­ä¹‰å‚è€ƒ"""
        try:
            cross_references = []
            
            # è·å–æ‰€æœ‰æ•°æ®åº“
            databases = await self.metadata_manager.get_databases_by_instance(instance_name, instance_id)
            
            for db in databases:
                db_name = db["database_name"]
                
                # è·å–æ•°æ®åº“ä¸­çš„æ‰€æœ‰é›†åˆ
                collections = await self.metadata_manager.get_collections_by_database(
                    instance_name, instance_id, db_name
                )
                
                for collection in collections:
                    collection_name = collection["collection_name"]
                    
                    # è·³è¿‡å½“å‰é›†åˆ
                    if db_name == exclude_db and collection_name == exclude_collection:
                        continue
                    
                    # è·å–é›†åˆä¸­çš„å­—æ®µ
                    fields = await self.metadata_manager.get_fields_by_collection(
                        instance_name, instance_id, db_name, collection_name
                    )
                    
                    # åŒ¹é…å­—æ®µ
                    relevant_fields = self._match_fields_to_keywords(fields, keywords)
                    
                    for field in relevant_fields:
                        if field.get("business_meaning"):  # åªè€ƒè™‘å·²å®šä¹‰è¯­ä¹‰çš„å­—æ®µ
                            cross_references.append({
                                "database_name": db_name,
                                "collection_name": collection_name,
                                "field_path": field["field_path"],
                                "business_meaning": field["business_meaning"],
                                "relevance_score": field["relevance_score"]
                            })
            
            # æŒ‰ç›¸å…³åº¦æ’åº
            cross_references.sort(key=lambda x: x["relevance_score"], reverse=True)
            
            return cross_references
            
        except Exception as e:
            logger.error("æŸ¥æ‰¾è·¨åº“è¡¨å‚è€ƒå¤±è´¥", error=str(e))
            return []
    
    async def _generate_completion_suggestions(
        self, relevant_fields: List[Dict[str, Any]], cross_references: List[Dict[str, Any]], 
        keywords: List[str]
    ) -> List[Dict[str, Any]]:
        """ç”Ÿæˆè¯­ä¹‰è¡¥å…¨å»ºè®®"""
        suggestions = []
        
        # ä¸ºæœªå®šä¹‰è¯­ä¹‰çš„ç›¸å…³å­—æ®µç”Ÿæˆå»ºè®®
        for field in relevant_fields:
            if not field.get("business_meaning"):
                field_path = field["field_path"]
                
                # ä»è·¨åº“è¡¨å‚è€ƒä¸­å¯»æ‰¾æœ€ç›¸ä¼¼çš„è¯­ä¹‰
                best_match = None
                best_score = 0.0
                
                for ref in cross_references:
                    # è®¡ç®—å­—æ®µåç›¸ä¼¼åº¦
                    similarity = self._calculate_field_similarity(field_path, ref["field_path"])
                    combined_score = (similarity + ref["relevance_score"]) / 2
                    
                    if combined_score > best_score:
                        best_score = combined_score
                        best_match = ref
                
                if best_match and best_score > 0.5:
                    suggestions.append({
                        "field_path": field_path,
                        "suggested_meaning": best_match["business_meaning"],
                        "confidence": best_score,
                        "reasoning": f"å‚è€ƒ {best_match['database_name']}.{best_match['collection_name']}.{best_match['field_path']}"
                    })
        
        # æŒ‰ç½®ä¿¡åº¦æ’åº
        suggestions.sort(key=lambda x: x["confidence"], reverse=True)
        
        return suggestions