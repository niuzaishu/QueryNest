# -*- coding: utf-8 -*-
"""шпнф╣ЙшбехЕих╖ехЕ╖

х╜УхЕГцХ░цНох║УцЙ╛ф╕НхИ░шпнф╣ЙцЧ╢я╝Мх░ЭшпХшзгшп╗хЕ╢ф╗Цх║Ушбиш┐ЫшбМшпнф╣ЙшбехЕихТМчФицИ╖чбошод
"""

from typing import Dict, List, Any, Optional, Tuple
import structlog
from mcp.types import Tool, TextContent
import re
from datetime import datetime

from database.connection_manager import ConnectionManager
from database.metadata_manager import MetadataManager
from scanner.semantic_analyzer import SemanticAnalyzer


logger = structlog.get_logger(__name__)


class SemanticCompletionTool:
    """шпнф╣ЙшбехЕих╖ехЕ╖"""
    
    def __init__(self, connection_manager: ConnectionManager, 
                 metadata_manager: MetadataManager,
                 semantic_analyzer: SemanticAnalyzer):
        self.connection_manager = connection_manager
        self.metadata_manager = metadata_manager
        self.semantic_analyzer = semantic_analyzer
    
    def get_tool_definition(self) -> Tool:
        """шО╖хПЦх╖ехЕ╖хоЪф╣Й"""
        return Tool(
            name="semantic_completion",
            description="х╜УхЕГцХ░цНох║УцЙ╛ф╕НхИ░шпнф╣ЙцЧ╢я╝Мх░ЭшпХшзгшп╗хЕ╢ф╗Цх║Ушбиш┐ЫшбМшпнф╣ЙшбехЕихТМчФицИ╖чбошод",
            inputSchema={
                "type": "object",
                "properties": {
                    "action": {
                        "type": "string",
                        "enum": ["analyze_unknown_field", "cross_reference_analysis", "suggest_semantics", "confirm_semantics"],
                        "description": "цУНф╜Ьч▒╗хЮЛя╝Ъanalyze_unknown_field=хИЖцЮРцЬкчЯехнЧцо╡я╝Мcross_reference_analysis=ш╖их║УшбихИЖцЮРя╝Мsuggest_semantics=шпнф╣Йх╗║шооя╝Мconfirm_semantics=чбошодшпнф╣Й"
                    },
                    "instance_name": {
                        "type": "string",
                        "description": "хоЮф╛ЛхРНчз░"
                    },
                    "database_name": {
                        "type": "string",
                        "description": "цХ░цНох║УхРНчз░"
                    },
                    "collection_name": {
                        "type": "string",
                        "description": "щЫЖхРИхРНчз░"
                    },
                    "field_path": {
                        "type": "string",
                        "description": "хнЧцо╡ш╖пх╛Дя╝ИчФиф║Оanalyze_unknown_fieldхТМconfirm_semanticsя╝Й"
                    },
                    "query_description": {
                        "type": "string",
                        "description": "чФицИ╖цЯешпвцППш┐░я╝ИчФиф║Оcross_reference_analysisя╝Й"
                    },
                    "suggested_meaning": {
                        "type": "string",
                        "description": "х╗║шоочЪДшпнф╣ЙхРлф╣Йя╝ИчФиф║Оconfirm_semanticsя╝Й"
                    },
                    "confidence_threshold": {
                        "type": "number",
                        "default": 0.6,
                        "description": "ч╜оф┐бх║жщШИхА╝я╝Мф╜Оф║ОцндхА╝чЪДх╗║шоощЬАшжБчФицИ╖чбошод"
                    }
                },
                "required": ["action", "instance_name"]
            }
        )
    
    async def execute(self, arguments: Dict[str, Any]) -> List[TextContent]:
        """цЙзшбМшпнф╣ЙшбехЕицУНф╜Ь"""
        try:
            action = arguments["action"]
            instance_name = arguments["instance_name"]
            
            # чбоф┐ЭхоЮф╛ЛхЕГцХ░цНох╖▓хИЭхзЛхМЦ
            if not await self.metadata_manager.init_instance_metadata(instance_name):
                return [TextContent(
                    type="text",
                    text=f"цЧац│ХхИЭхзЛхМЦхоЮф╛Л '{instance_name}' чЪДхЕГцХ░цНох║У"
                )]
            
            if action == "analyze_unknown_field":
                return await self._analyze_unknown_field(arguments)
            elif action == "cross_reference_analysis":
                return await self._cross_reference_analysis(arguments)
            elif action == "suggest_semantics":
                return await self._suggest_semantics(arguments)
            elif action == "confirm_semantics":
                return await self._confirm_semantics(arguments)
            else:
                return [TextContent(
                    type="text",
                    text=f"ф╕НцФпцМБчЪДцУНф╜Ьч▒╗хЮЛ: {action}"
                )]
                
        except Exception as e:
            logger.error("шпнф╣ЙшбехЕицУНф╜Ьхд▒ш┤е", error=str(e))
            return [TextContent(
                type="text",
                text=f"шпнф╣ЙшбехЕицУНф╜Ьхд▒ш┤е: {str(e)}"
            )]
    
    async def _analyze_unknown_field(self, arguments: Dict[str, Any]) -> List[TextContent]:
        """хИЖцЮРцЬкчЯехнЧцо╡чЪДшпнф╣Й"""
        instance_name = arguments["instance_name"]
        database_name = arguments["database_name"]
        collection_name = arguments["collection_name"]
        field_path = arguments["field_path"]
        confidence_threshold = arguments.get("confidence_threshold", 0.6)
        
        try:
            # шО╖хПЦхоЮф╛Лф┐бцБп
            instance_info = await self.metadata_manager.get_instance_by_name(instance_name, instance_name)
            if not instance_info:
                return [TextContent(
                    type="text",
                    text=f"хоЮф╛Л '{instance_name}' ф╕НхнШхЬи"
                )]
            
            instance_id = instance_info["_id"]
            
            # шО╖хПЦхнЧцо╡ф┐бцБп
            fields = await self.metadata_manager.get_fields_by_collection(
                instance_name, instance_id, database_name, collection_name
            )
            
            target_field = None
            for field in fields:
                if field["field_path"] == field_path:
                    target_field = field
                    break
            
            if not target_field:
                return [TextContent(
                    type="text",
                    text=f"хнЧцо╡ '{field_path}' хЬищЫЖхРИ '{database_name}.{collection_name}' ф╕нф╕НхнШхЬи"
                )]
            
            # цгАцЯецШпхРжх╖▓цЬЙшпнф╣ЙхоЪф╣Й
            if target_field.get("business_meaning"):
                return [TextContent(
                    type="text",
                    text=f"хнЧцо╡ '{field_path}' х╖▓цЬЙшпнф╣ЙхоЪф╣Й: {target_field['business_meaning']}"
                )]
            
            result_text = f"## цЬкчЯехнЧцо╡шпнф╣ЙхИЖцЮР: {field_path}\n\n"
            
            # 1. хЯ║чбАшпнф╣ЙхИЖцЮР
            analysis = await self.semantic_analyzer.analyze_field_semantics(
                instance_name, database_name, collection_name, field_path, target_field
            )
            
            result_text += f"### хЯ║чбАхИЖцЮРч╗УцЮЬ\n\n"
            result_text += f"- **х╗║шоохРлф╣Й**: {analysis['suggested_meaning']}\n"
            result_text += f"- **ч╜оф┐бх║ж**: {analysis['confidence']:.1%}\n"
            result_text += f"- **хИЖцЮРф╛ЭцНо**: {', '.join(analysis['reasoning'])}\n\n"
            
            # 2. ш╖их║УшбичЫ╕ф╝╝хнЧцо╡хИЖцЮР
            similar_fields = await self._find_similar_fields_across_collections(
                instance_name, instance_id, field_path, database_name, collection_name
            )
            
            if similar_fields:
                result_text += f"### ш╖их║УшбичЫ╕ф╝╝хнЧцо╡\n\n"
                for similar in similar_fields[:5]:  # цШ╛чд║хЙН5ф╕кцЬАчЫ╕ф╝╝чЪД
                    similarity = similar['similarity']
                    db_name = similar['database_name']
                    coll_name = similar['collection_name']
                    similar_path = similar['field_path']
                    meaning = similar.get('business_meaning', 'цЬкхоЪф╣Й')
                    
                    result_text += f"- **{db_name}.{coll_name}.{similar_path}** (чЫ╕ф╝╝х║ж: {similarity:.1%})\n"
                    result_text += f"  шпнф╣Й: {meaning}\n\n"
            
            # 3. хА╝цибх╝ПхИЖцЮР
            value_patterns = await self._analyze_value_patterns(
                instance_name, instance_id, target_field
            )
            
            if value_patterns:
                result_text += f"### хА╝цибх╝ПхИЖцЮР\n\n"
                for pattern in value_patterns:
                    result_text += f"- **{pattern['pattern']}**: {pattern['meaning']} (хМ╣щЕНх║ж: {pattern['match_rate']:.1%})\n"
                result_text += "\n"
            
            # 4. чФЯцИРцЬАч╗Их╗║шоо
            final_suggestions = await self._generate_final_suggestions(
                analysis, similar_fields, value_patterns, confidence_threshold
            )
            
            result_text += f"### шпнф╣Йх╗║шоо\n\n"
            
            if analysis['confidence'] >= confidence_threshold:
                result_text += f"тЬЕ **щлШч╜оф┐бх║жх╗║шоо**: {analysis['suggested_meaning']}\n\n"
                result_text += f"х╗║шоочЫ┤цОещЗЗчФицндшпнф╣ЙхоЪф╣ЙуАВ\n\n"
            else:
                result_text += f"тЪая╕П **щЬАшжБчбошодчЪДх╗║шоо**:\n\n"
                for i, suggestion in enumerate(final_suggestions, 1):
                    result_text += f"{i}. **{suggestion['meaning']}** (ч╜оф┐бх║ж: {suggestion['confidence']:.1%})\n"
                    result_text += f"   ф╛ЭцНо: {suggestion['reasoning']}\n\n"
                
                result_text += f"ЁЯТб **ф╕Лф╕АцнецУНф╜Ь**:\n"
                result_text += f"ф╜┐чФи `semantic_completion` х╖ехЕ╖чЪД `confirm_semantics` цУНф╜Ьчбошодшпнф╣ЙхоЪф╣ЙуАВ\n\n"
            
            return [TextContent(type="text", text=result_text)]
            
        except Exception as e:
            logger.error("хИЖцЮРцЬкчЯехнЧцо╡хд▒ш┤е", error=str(e))
            return [TextContent(
                type="text",
                text=f"хИЖцЮРцЬкчЯехнЧцо╡цЧ╢хПСчФЯщФЩшпп: {str(e)}"
            )]
    
    async def _cross_reference_analysis(self, arguments: Dict[str, Any]) -> List[TextContent]:
        """ш╖их║УшбихИЖцЮРя╝Мца╣цНоцЯешпвцППш┐░цОицЦнхПпшГ╜чЪДхнЧцо╡шпнф╣Й"""
        instance_name = arguments["instance_name"]
        database_name = arguments["database_name"]
        collection_name = arguments["collection_name"]
        query_description = arguments["query_description"]
        
        try:
            # шО╖хПЦхоЮф╛Лф┐бцБп
            instance_info = await self.metadata_manager.get_instance_by_name(instance_name, instance_name)
            if not instance_info:
                return [TextContent(
                    type="text",
                    text=f"хоЮф╛Л '{instance_name}' ф╕НхнШхЬи"
                )]
            
            instance_id = instance_info["_id"]
            
            result_text = f"## ш╖их║Ушбишпнф╣ЙхИЖцЮР\n\n"
            result_text += f"**цЯешпвцППш┐░**: {query_description}\n\n"
            
            # 1. цПРхПЦцЯешпвф╕нчЪДхЕ│щФошпН
            keywords = self._extract_query_keywords(query_description)
            result_text += f"### цПРхПЦчЪДхЕ│щФошпН\n\n"
            result_text += f"{', '.join(keywords)}\n\n"
            
            # 2. хЬих╜УхЙНщЫЖхРИф╕нцЯецЙ╛чЫ╕хЕ│хнЧцо╡
            current_fields = await self.metadata_manager.get_fields_by_collection(
                instance_name, instance_id, database_name, collection_name
            )
            
            relevant_fields = self._match_fields_to_keywords(current_fields, keywords)
            
            if relevant_fields:
                result_text += f"### х╜УхЙНщЫЖхРИчЫ╕хЕ│хнЧцо╡\n\n"
                for field in relevant_fields:
                    field_path = field['field_path']
                    meaning = field.get('business_meaning', 'цЬкхоЪф╣Й')
                    relevance = field['relevance_score']
                    
                    result_text += f"- **{field_path}** (чЫ╕хЕ│х║ж: {relevance:.1%})\n"
                    result_text += f"  х╜УхЙНшпнф╣Й: {meaning}\n\n"
            
            # 3. ш╖их║УшбицЯецЙ╛чЫ╕ф╝╝шпнф╣Й
            cross_references = await self._find_cross_references(
                instance_name, instance_id, keywords, database_name, collection_name
            )
            
            if cross_references:
                result_text += f"### ш╖их║УшбихПВшАГ\n\n"
                for ref in cross_references[:10]:  # цШ╛чд║хЙН10ф╕к
                    db_name = ref['database_name']
                    coll_name = ref['collection_name']
                    field_path = ref['field_path']
                    meaning = ref['business_meaning']
                    relevance = ref['relevance_score']
                    
                    result_text += f"- **{db_name}.{coll_name}.{field_path}** (чЫ╕хЕ│х║ж: {relevance:.1%})\n"
                    result_text += f"  шпнф╣Й: {meaning}\n\n"
            
            # 4. чФЯцИРшпнф╣ЙшбехЕих╗║шоо
            completion_suggestions = await self._generate_completion_suggestions(
                relevant_fields, cross_references, keywords
            )
            
            if completion_suggestions:
                result_text += f"### шпнф╣ЙшбехЕих╗║шоо\n\n"
                for i, suggestion in enumerate(completion_suggestions, 1):
                    field_path = suggestion['field_path']
                    suggested_meaning = suggestion['suggested_meaning']
                    confidence = suggestion['confidence']
                    reasoning = suggestion['reasoning']
                    
                    result_text += f"{i}. **хнЧцо╡**: {field_path}\n"
                    result_text += f"   **х╗║шоошпнф╣Й**: {suggested_meaning}\n"
                    result_text += f"   **ч╜оф┐бх║ж**: {confidence:.1%}\n"
                    result_text += f"   **ф╛ЭцНо**: {reasoning}\n\n"
                
                result_text += f"ЁЯТб **ф╕Лф╕АцнецУНф╜Ь**:\n"
                result_text += f"ф╜┐чФи `semantic_completion` х╖ехЕ╖чЪД `confirm_semantics` цУНф╜Ьчбошодш┐Щф║Ышпнф╣ЙхоЪф╣ЙуАВ\n\n"
            else:
                result_text += f"### цЬкцЙ╛хИ░чЫ╕хЕ│чЪДшпнф╣ЙхПВшАГ\n\n"
                result_text += f"х╗║шооя╝Ъ\n"
                result_text += f"1. цгАцЯецЯешпвцППш┐░цШпхРжхЗЖчбо\n"
                result_text += f"2. х░ЭшпХф╜┐чФицЫ┤хЕ╖ф╜УчЪДхЕ│щФошпН\n"
                result_text += f"3. цЙЛхКихоЪф╣ЙхнЧцо╡шпнф╣Й\n\n"
            
            return [TextContent(type="text", text=result_text)]
            
        except Exception as e:
            logger.error("ш╖их║УшбихИЖцЮРхд▒ш┤е", error=str(e))
            return [TextContent(
                type="text",
                text=f"ш╖их║УшбихИЖцЮРцЧ╢хПСчФЯщФЩшпп: {str(e)}"
            )]
    
    async def _suggest_semantics(self, arguments: Dict[str, Any]) -> List[TextContent]:
        """ф╕║цМЗхоЪщЫЖхРИчЪДцЙАцЬЙцЬкхоЪф╣Йшпнф╣ЙхнЧцо╡цПРф╛Ых╗║шоо"""
        instance_name = arguments["instance_name"]
        database_name = arguments["database_name"]
        collection_name = arguments["collection_name"]
        confidence_threshold = arguments.get("confidence_threshold", 0.6)
        
        try:
            # шО╖хПЦхоЮф╛Лф┐бцБп
            instance_info = await self.metadata_manager.get_instance_by_name(instance_name, instance_name)
            if not instance_info:
                return [TextContent(
                    type="text",
                    text=f"хоЮф╛Л '{instance_name}' ф╕НхнШхЬи"
                )]
            
            instance_id = instance_info["_id"]
            
            # шО╖хПЦщЫЖхРИф╕нчЪДцЙАцЬЙхнЧцо╡
            fields = await self.metadata_manager.get_fields_by_collection(
                instance_name, instance_id, database_name, collection_name
            )
            
            # чнЫщАЙхЗ║цЬкхоЪф╣Йшпнф╣ЙчЪДхнЧцо╡
            undefined_fields = [f for f in fields if not f.get("business_meaning")]
            
            if not undefined_fields:
                return [TextContent(
                    type="text",
                    text=f"щЫЖхРИ '{database_name}.{collection_name}' ф╕нчЪДцЙАцЬЙхнЧцо╡щГ╜х╖▓хоЪф╣Йшпнф╣Й"
                )]
            
            result_text = f"## шпнф╣Йх╗║шоо: {database_name}.{collection_name}\n\n"
            result_text += f"хПСчО░ {len(undefined_fields)} ф╕кцЬкхоЪф╣Йшпнф╣ЙчЪДхнЧцо╡\n\n"
            
            high_confidence_suggestions = []
            low_confidence_suggestions = []
            
            for field in undefined_fields:
                field_path = field["field_path"]
                
                # хИЖцЮРхнЧцо╡шпнф╣Й
                analysis = await self.semantic_analyzer.analyze_field_semantics(
                    instance_name, database_name, collection_name, field_path, field
                )
                
                suggestion = {
                    "field_path": field_path,
                    "suggested_meaning": analysis["suggested_meaning"],
                    "confidence": analysis["confidence"],
                    "reasoning": analysis["reasoning"]
                }
                
                if analysis["confidence"] >= confidence_threshold:
                    high_confidence_suggestions.append(suggestion)
                else:
                    low_confidence_suggestions.append(suggestion)
            
            # цШ╛чд║щлШч╜оф┐бх║жх╗║шоо
            if high_confidence_suggestions:
                result_text += f"### ЁЯОп щлШч╜оф┐бх║жх╗║шоо (тЙе{confidence_threshold:.0%})\n\n"
                for suggestion in high_confidence_suggestions:
                    field_path = suggestion["field_path"]
                    meaning = suggestion["suggested_meaning"]
                    confidence = suggestion["confidence"]
                    
                    result_text += f"- **{field_path}**: {meaning} (ч╜оф┐бх║ж: {confidence:.1%})\n"
                
                result_text += f"\nш┐Щф║Ых╗║шоохПпф╗ечЫ┤цОещЗЗчФиуАВ\n\n"
            
            # цШ╛чд║ф╜Оч╜оф┐бх║жх╗║шоо
            if low_confidence_suggestions:
                result_text += f"### тЪая╕П щЬАшжБчбошодчЪДх╗║шоо (<{confidence_threshold:.0%})\n\n"
                for suggestion in low_confidence_suggestions:
                    field_path = suggestion["field_path"]
                    meaning = suggestion["suggested_meaning"]
                    confidence = suggestion["confidence"]
                    reasoning = ", ".join(suggestion["reasoning"])
                    
                    result_text += f"- **{field_path}**: {meaning}\n"
                    result_text += f"  ч╜оф┐бх║ж: {confidence:.1%} | ф╛ЭцНо: {reasoning}\n\n"
            
            # цУНф╜Ьх╗║шоо
            result_text += f"### ЁЯТб цУНф╜Ьх╗║шоо\n\n"
            
            if high_confidence_suggestions:
                result_text += f"1. **цЙ╣щЗПчбошодщлШч╜оф┐бх║жх╗║шоо**:\n"
                for suggestion in high_confidence_suggestions:
                    field_path = suggestion["field_path"]
                    meaning = suggestion["suggested_meaning"]
                    result_text += f"   - чбошод `{field_path}` ф╕║ \"{meaning}\"\n"
                result_text += "\n"
            
            if low_confidence_suggestions:
                result_text += f"2. **щАРф╕кчбошодф╜Оч╜оф┐бх║жх╗║шоо**:\n"
                result_text += f"   ф╜┐чФи `semantic_completion` х╖ехЕ╖чЪД `confirm_semantics` цУНф╜Ь\n\n"
            
            return [TextContent(type="text", text=result_text)]
            
        except Exception as e:
            logger.error("чФЯцИРшпнф╣Йх╗║шоохд▒ш┤е", error=str(e))
            return [TextContent(
                type="text",
                text=f"чФЯцИРшпнф╣Йх╗║шооцЧ╢хПСчФЯщФЩшпп: {str(e)}"
            )]
    
    async def _confirm_semantics(self, arguments: Dict[str, Any]) -> List[TextContent]:
        """чбошодх╣╢ф┐ЭхнШхнЧцо╡шпнф╣Й"""
        instance_name = arguments["instance_name"]
        database_name = arguments["database_name"]
        collection_name = arguments["collection_name"]
        field_path = arguments["field_path"]
        suggested_meaning = arguments["suggested_meaning"]
        
        try:
            # шО╖хПЦхоЮф╛Лф┐бцБп
            instance_info = await self.metadata_manager.get_instance_by_name(instance_name, instance_name)
            if not instance_info:
                return [TextContent(
                    type="text",
                    text=f"хоЮф╛Л '{instance_name}' ф╕НхнШхЬи"
                )]
            
            instance_id = instance_info["_id"]
            
            # цЫ┤цЦ░хнЧцо╡шпнф╣Й
            success = await self.metadata_manager.update_field_semantics(
                instance_name, instance_id, database_name, collection_name, 
                field_path, suggested_meaning
            )
            
            if success:
                result_text = f"тЬЕ **шпнф╣ЙчбошодцИРхКЯ**\n\n"
                result_text += f"- **хнЧцо╡**: {database_name}.{collection_name}.{field_path}\n"
                result_text += f"- **шпнф╣Й**: {suggested_meaning}\n"
                result_text += f"- **цЫ┤цЦ░цЧ╢щЧ┤**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
                result_text += f"шпнф╣ЙхоЪф╣Йх╖▓ф┐ЭхнШхИ░хЕГцХ░цНох║Уф╕нуАВ\n"
                
                return [TextContent(type="text", text=result_text)]
            else:
                return [TextContent(
                    type="text",
                    text=f"тЭМ шпнф╣Йчбошодхд▒ш┤ея╝ЪцЧац│ХцЫ┤цЦ░хнЧцо╡ '{field_path}' чЪДшпнф╣Йф┐бцБп"
                )]
                
        except Exception as e:
            logger.error("чбошодшпнф╣Йхд▒ш┤е", error=str(e))
            return [TextContent(
                type="text",
                text=f"чбошодшпнф╣ЙцЧ╢хПСчФЯщФЩшпп: {str(e)}"
            )]
    
    async def _find_similar_fields_across_collections(
        self, instance_name: str, instance_id: str, field_path: str, 
        exclude_db: str, exclude_collection: str
    ) -> List[Dict[str, Any]]:
        """хЬихЕ╢ф╗ЦщЫЖхРИф╕нцЯецЙ╛чЫ╕ф╝╝чЪДхнЧцо╡"""
        try:
            # шО╖хПЦцЙАцЬЙцХ░цНох║У
            databases = await self.metadata_manager.get_databases_by_instance(instance_name, instance_id)
            
            similar_fields = []
            
            for db in databases:
                db_name = db["database_name"]
                
                # шО╖хПЦцХ░цНох║Уф╕нчЪДцЙАцЬЙщЫЖхРИ
                collections = await self.metadata_manager.get_collections_by_database(
                    instance_name, instance_id, db_name
                )
                
                for collection in collections:
                    collection_name = collection["collection_name"]
                    
                    # ш╖│ш┐Зх╜УхЙНщЫЖхРИ
                    if db_name == exclude_db and collection_name == exclude_collection:
                        continue
                    
                    # шО╖хПЦщЫЖхРИф╕нчЪДхнЧцо╡
                    fields = await self.metadata_manager.get_fields_by_collection(
                        instance_name, instance_id, db_name, collection_name
                    )
                    
                    for field in fields:
                        # шобчоЧхнЧцо╡хРНчз░чЫ╕ф╝╝х║ж
                        similarity = self._calculate_field_similarity(field_path, field["field_path"])
                        
                        if similarity > 0.5:  # чЫ╕ф╝╝х║жщШИхА╝
                            similar_fields.append({
                                "database_name": db_name,
                                "collection_name": collection_name,
                                "field_path": field["field_path"],
                                "business_meaning": field.get("business_meaning"),
                                "similarity": similarity,
                                "field_type": field.get("field_type")
                            })
            
            # цМЙчЫ╕ф╝╝х║жцОТх║П
            similar_fields.sort(key=lambda x: x["similarity"], reverse=True)
            
            return similar_fields
            
        except Exception as e:
            logger.error("цЯецЙ╛чЫ╕ф╝╝хнЧцо╡хд▒ш┤е", error=str(e))
            return []
    
    def _calculate_field_similarity(self, field1: str, field2: str) -> float:
        """шобчоЧф╕дф╕кхнЧцо╡хРНчз░чЪДчЫ╕ф╝╝х║ж"""
        # чоАхНХчЪДхнЧчмжф╕▓чЫ╕ф╝╝х║жшобчоЧ
        field1_clean = field1.lower().replace('_', '').replace('-', '')
        field2_clean = field2.lower().replace('_', '').replace('-', '')
        
        # хоМхЕихМ╣щЕН
        if field1_clean == field2_clean:
            return 1.0
        
        # хМЕхРлхЕ│ч│╗
        if field1_clean in field2_clean or field2_clean in field1_clean:
            return 0.8
        
        # ч╝Цш╛Сш╖Эчж╗чЫ╕ф╝╝х║ж
        max_len = max(len(field1_clean), len(field2_clean))
        if max_len == 0:
            return 0.0
        
        edit_distance = self._levenshtein_distance(field1_clean, field2_clean)
        similarity = 1.0 - (edit_distance / max_len)
        
        return max(0.0, similarity)
    
    def _levenshtein_distance(self, s1: str, s2: str) -> int:
        """шобчоЧч╝Цш╛Сш╖Эчж╗"""
        if len(s1) < len(s2):
            return self._levenshtein_distance(s2, s1)
        
        if len(s2) == 0:
            return len(s1)
        
        previous_row = list(range(len(s2) + 1))
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row
        
        return previous_row[-1]
    
    async def _analyze_value_patterns(self, instance_name: str, instance_id: str, field_info: Dict[str, Any]) -> List[Dict[str, Any]]:
        """хИЖцЮРхнЧцо╡хА╝чЪДцибх╝П"""
        patterns = []
        examples = field_info.get("examples", [])
        
        if not examples:
            return patterns
        
        # х╕╕шзБцибх╝ПцгАц╡Л
        pattern_checks = [
            (r'^\d{11}$', 'цЙЛцЬ║хП╖чаБ'),
            (r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$', 'щВочо▒хЬ░хЭА'),
            (r'^https?://', 'URLхЬ░хЭА'),
            (r'^\d{4}-\d{2}-\d{2}', 'цЧецЬЯца╝х╝П'),
            (r'^[0-9a-fA-F]{24}$', 'MongoDB ObjectId'),
            (r'^\d{13}$', 'цЧ╢щЧ┤цИ│я╝ИцплчзТя╝Й'),
            (r'^\d{10}$', 'цЧ╢щЧ┤цИ│я╝ИчзТя╝Й'),
            (r'^[0-9]{6}$', 'щкМшпБчаБ/щВоцФ┐ч╝ЦчаБ'),
        ]
        
        for pattern_regex, meaning in pattern_checks:
            matches = 0
            for example in examples:
                if example is not None and re.match(pattern_regex, str(example)):
                    matches += 1
            
            if matches > 0:
                match_rate = matches / len(examples)
                patterns.append({
                    "pattern": pattern_regex,
                    "meaning": meaning,
                    "match_rate": match_rate,
                    "matches": matches,
                    "total": len(examples)
                })
        
        # цМЙхМ╣щЕНчОЗцОТх║П
        patterns.sort(key=lambda x: x["match_rate"], reverse=True)
        
        return patterns
    
    async def _generate_final_suggestions(
        self, analysis: Dict[str, Any], similar_fields: List[Dict[str, Any]], 
        value_patterns: List[Dict[str, Any]], confidence_threshold: float
    ) -> List[Dict[str, Any]]:
        """чФЯцИРцЬАч╗ИчЪДшпнф╣Йх╗║шоо"""
        suggestions = []
        
        # хЯ║чбАхИЖцЮРх╗║шоо
        if analysis["suggested_meaning"]:
            suggestions.append({
                "meaning": analysis["suggested_meaning"],
                "confidence": analysis["confidence"],
                "reasoning": "хЯ║ф║ОхнЧцо╡хРНчз░хТМч▒╗хЮЛхИЖцЮР"
            })
        
        # чЫ╕ф╝╝хнЧцо╡х╗║шоо
        for similar in similar_fields[:3]:  # хПЦхЙН3ф╕кцЬАчЫ╕ф╝╝чЪД
            if similar.get("business_meaning"):
                confidence = similar["similarity"] * 0.8  # чЫ╕ф╝╝х║жцКШцЙг
                suggestions.append({
                    "meaning": similar["business_meaning"],
                    "confidence": confidence,
                    "reasoning": f"хПВшАГчЫ╕ф╝╝хнЧцо╡ {similar['database_name']}.{similar['collection_name']}.{similar['field_path']}"
                })
        
        # хА╝цибх╝Пх╗║шоо
        for pattern in value_patterns[:2]:  # хПЦхЙН2ф╕кцЬАхМ╣щЕНчЪДцибх╝П
            if pattern["match_rate"] > 0.7:  # щлШхМ╣щЕНчОЗ
                confidence = pattern["match_rate"] * 0.9
                suggestions.append({
                    "meaning": pattern["meaning"],
                    "confidence": confidence,
                    "reasoning": f"хЯ║ф║ОхА╝цибх╝ПхИЖцЮРя╝ИхМ╣щЕНчОЗ: {pattern['match_rate']:.1%}я╝Й"
                })
        
        # хО╗щЗНх╣╢цОТх║П
        unique_suggestions = []
        seen_meanings = set()
        
        for suggestion in suggestions:
            meaning = suggestion["meaning"]
            if meaning not in seen_meanings:
                unique_suggestions.append(suggestion)
                seen_meanings.add(meaning)
        
        # цМЙч╜оф┐бх║жцОТх║П
        unique_suggestions.sort(key=lambda x: x["confidence"], reverse=True)
        
        return unique_suggestions[:5]  # ш┐ФхЫЮхЙН5ф╕кх╗║шоо
    
    def _extract_query_keywords(self, query_description: str) -> List[str]:
        """ф╗ОцЯешпвцППш┐░ф╕нцПРхПЦхЕ│щФошпН"""
        # чоАхНХчЪДхЕ│щФошпНцПРхПЦ
        import jieba
        
        # ф╕нцЦЗхИЖшпН
        words = list(jieba.cut(query_description))
        
        # ш┐Зц╗дхБЬчФишпНхТМчЯншпН
        stop_words = {'чЪД', 'цШп', 'хЬи', 'цЬЙ', 'хТМ', 'ф╕О', 'цИЦ', 'ф╜Ж', 'шАМ', 'ф║Ж', 'чЭА', 'ш┐З', 'шжБ', 'ф╝Ъ', 'шГ╜', 'хПпф╗е', 'х║Фшпе'}
        keywords = [word.strip() for word in words if len(word.strip()) > 1 and word.strip() not in stop_words]
        
        # шЛ▒цЦЗхНХшпНцПРхПЦ
        english_words = re.findall(r'\b[a-zA-Z]+\b', query_description)
        keywords.extend([word.lower() for word in english_words if len(word) > 2])
        
        return list(set(keywords))  # хО╗щЗН
    
    def _match_fields_to_keywords(self, fields: List[Dict[str, Any]], keywords: List[str]) -> List[Dict[str, Any]]:
        """х░ЖхнЧцо╡ф╕ОхЕ│щФошпНхМ╣щЕН"""
        relevant_fields = []
        
        for field in fields:
            field_path = field["field_path"].lower()
            business_meaning = field.get("business_meaning", "").lower()
            
            relevance_score = 0.0
            
            for keyword in keywords:
                keyword_lower = keyword.lower()
                
                # хнЧцо╡хРНхМ╣щЕН
                if keyword_lower in field_path:
                    relevance_score += 0.8
                
                # шпнф╣ЙхМ╣щЕН
                if keyword_lower in business_meaning:
                    relevance_score += 1.0
                
                # щГихИЖхМ╣щЕН
                if any(keyword_lower in part for part in field_path.split('_')):
                    relevance_score += 0.5
            
            if relevance_score > 0:
                field_copy = field.copy()
                field_copy["relevance_score"] = min(relevance_score, 1.0)
                relevant_fields.append(field_copy)
        
        # цМЙчЫ╕хЕ│х║жцОТх║П
        relevant_fields.sort(key=lambda x: x["relevance_score"], reverse=True)
        
        return relevant_fields
    
    async def _find_cross_references(
        self, instance_name: str, instance_id: str, keywords: List[str], 
        exclude_db: str, exclude_collection: str
    ) -> List[Dict[str, Any]]:
        """цЯецЙ╛ш╖их║УшбичЪДшпнф╣ЙхПВшАГ"""
        try:
            cross_references = []
            
            # шО╖хПЦцЙАцЬЙцХ░цНох║У
            databases = await self.metadata_manager.get_databases_by_instance(instance_name, instance_id)
            
            for db in databases:
                db_name = db["database_name"]
                
                # шО╖хПЦцХ░цНох║Уф╕нчЪДцЙАцЬЙщЫЖхРИ
                collections = await self.metadata_manager.get_collections_by_database(
                    instance_name, instance_id, db_name
                )
                
                for collection in collections:
                    collection_name = collection["collection_name"]
                    
                    # ш╖│ш┐Зх╜УхЙНщЫЖхРИ
                    if db_name == exclude_db and collection_name == exclude_collection:
                        continue
                    
                    # шО╖хПЦщЫЖхРИф╕нчЪДхнЧцо╡
                    fields = await self.metadata_manager.get_fields_by_collection(
                        instance_name, instance_id, db_name, collection_name
                    )
                    
                    # хМ╣щЕНхнЧцо╡
                    relevant_fields = self._match_fields_to_keywords(fields, keywords)
                    
                    for field in relevant_fields:
                        if field.get("business_meaning"):  # хПкшАГшЩСх╖▓хоЪф╣Йшпнф╣ЙчЪДхнЧцо╡
                            cross_references.append({
                                "database_name": db_name,
                                "collection_name": collection_name,
                                "field_path": field["field_path"],
                                "business_meaning": field["business_meaning"],
                                "relevance_score": field["relevance_score"]
                            })
            
            # цМЙчЫ╕хЕ│х║жцОТх║П
            cross_references.sort(key=lambda x: x["relevance_score"], reverse=True)
            
            return cross_references
            
        except Exception as e:
            logger.error("цЯецЙ╛ш╖их║УшбихПВшАГхд▒ш┤е", error=str(e))
            return []
    
    async def _generate_completion_suggestions(
        self, relevant_fields: List[Dict[str, Any]], cross_references: List[Dict[str, Any]], 
        keywords: List[str]
    ) -> List[Dict[str, Any]]:
        """чФЯцИРшпнф╣ЙшбехЕих╗║шоо"""
        suggestions = []
        
        # ф╕║цЬкхоЪф╣Йшпнф╣ЙчЪДчЫ╕хЕ│хнЧцо╡чФЯцИРх╗║шоо
        for field in relevant_fields:
            if not field.get("business_meaning"):
                field_path = field["field_path"]
                
                # ф╗Ош╖их║УшбихПВшАГф╕нхп╗цЙ╛цЬАчЫ╕ф╝╝чЪДшпнф╣Й
                best_match = None
                best_score = 0.0
                
                for ref in cross_references:
                    # шобчоЧхнЧцо╡хРНчЫ╕ф╝╝х║ж
                    similarity = self._calculate_field_similarity(field_path, ref["field_path"])
                    combined_score = (similarity + ref["relevance_score"]) / 2
                    
                    if combined_score > best_score:
                        best_score = combined_score
                        best_match = ref
                
                if best_match and best_score > 0.5:
                    suggestions.append({
                        "field_path": field_path,
                        "suggested_meaning": best_match["business_meaning"],
                        "confidence": best_score,
                        "reasoning": f"хПВшАГ {best_match['database_name']}.{best_match['collection_name']}.{best_match['field_path']}"
                    })
        
        # цМЙч╜оф┐бх║жцОТх║П
        suggestions.sort(key=lambda x: x["confidence"], reverse=True)
        
        return suggestions